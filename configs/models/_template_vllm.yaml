# vLLM Model Template
# 
# Two modes available:
#   A) Local vLLM: Server auto-started by evaluation runner (client: vllm)
#   B) External vLLM: Connect to already-running vLLM server (client: openai)
#
# Usage:
#   1. Copy this file and rename it to your model name (e.g., Qwen3-4B.yaml)
#   2. Choose mode A or B below and configure accordingly
#   3. Run: uv run run_eval.py --config your-model-name

wandb:
  run_name: "Model-Name-Here"

metadata:
  release_date: "YYYY-MM-DD"
  size_category: "Small (<10B)"  # or "Medium (10-70B)", "Large (>70B)"
  model_size: 4000000000  # Number of parameters
  context_window: 32768
  max_output_tokens: 4096

# ============================================================
# Mode A: Local vLLM (auto-managed server)
# ============================================================
# model:
#   name: organization/model-name  # HuggingFace model ID
#   client: vllm                   # Enables auto server management
#   provider: custom
#   
#   params:
#     max_tokens: 4096
#     temperature: 0.0
#   
#   vllm:
#     tensor_parallel_size: 1
#     gpu_memory_utilization: 0.9
#     dtype: auto
#     max_model_len: 32768
#     port: 8000
#     host: "0.0.0.0"
#     # trust_remote_code: true
#     # quantization: awq
#     # reasoning_parser: deepseek_r1

# ============================================================
# Mode B: External vLLM (already running server, e.g. RunPod)
# ============================================================
model:
  name: your-served-model-name     # --served-model-name from vLLM server
  client: openai                   # OpenAI-compatible API
  provider: vllm
  base_url: https://your-endpoint.proxy.runpod.net/v1
  api_key_env: RUNPOD_API_KEY      # Environment variable name for API key
  
  params:
    max_tokens: 4096
    temperature: 0.0

# Benchmark-specific overrides
benchmarks:
  bfcl:
    use_native_tools: false
  ko_mtbench:
    temperature: 0.7






