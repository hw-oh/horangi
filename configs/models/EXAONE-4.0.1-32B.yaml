# =============================================================================
# Qwen3-4B-Instruct (RunPod vLLM)
# 
# vLLM 서버를 통해 오픈소스 모델을 서빙하는 설정 예시
# =============================================================================

# 모델 식별자 (표시용)
# vLLM은 HuggingFace 전체 경로를 모델명으로 사용
model_id: LGAI-EXAONE/EXAONE-4.0.1-32B

# API Provider: vLLM은 OpenAI 호환 API를 제공하므로 "openai" 지정
# → inspect eval에는 "openai/LGAI-EXAONE/EXAONE-4.0.1-32B"로 전달됨
api_provider: openai

# 모델 메타데이터 (선택사항, 문서화 및 분석용)
metadata:
  provider: LGAI-EXAONE  # 제공사
  name: EXAONE-4.0.1-32B  # 표시 이름
  release_date: "2025-07-29"  # 출시일
  description: "RunPod vLLM 서버에서 실행"
  context_window: 32768  # 컨텍스트 윈도우 크기 (토큰)
  max_output_tokens: 4096  # 최대 출력 토큰

# API 설정
base_url: https://lgeyzkj4kubj2u-8000.proxy.runpod.net/v1  # vLLM 서버 URL
api_key_env: RUNPOD_API_KEY  # API 키를 읽을 환경변수 이름 (vLLM 기본은 불필요)

# 기본 파라미터 (base_config의 defaults와 병합됨)
# 여기서 지정한 값이 base_config의 값을 override
defaults:
  temperature: 0.0  # 생성 온도 (0.0 = deterministic)
  max_tokens: 4096  # 최대 생성 토큰 수
  # top_p: 1.0  # Top-p 샘플링 (선택사항)

# 벤치마크별 오버라이드 설정 (선택사항)
# 특정 벤치마크에서만 다른 설정을 사용할 때
benchmarks:
  # BFCL: Function Calling 벤치마크
  # - use_native_tools: true → Native Tool Calling (OpenAI, Claude, Gemini 등)
  # - use_native_tools: false → Text-based (오픈소스 모델 권장)
  bfcl:
    use_native_tools: false
  
  # MT-Bench: 창의적 응답이 필요한 경우
  ko_mtbench:
    temperature: 0.7
