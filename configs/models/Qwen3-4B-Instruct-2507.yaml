# Qwen3-4B-Instruct (vLLM Native)
#
# Uses native vLLM support - server is automatically started and managed.
# No need to manually run `vllm serve` before evaluation.

wandb:
  run_name: "Qwen3-4B-Instruct-2507"

metadata:
  release_date: "2025-07-01"
  size_category: "Small (<10B)"
  model_size: 4000000000
  context_window: 32768
  max_output_tokens: 4096

model:
  name: Qwen/Qwen3-4B-Instruct-2507
  client: vllm  # Native vLLM support
  provider: qwen

  params:
    max_tokens: 4096
    temperature: 0.0

  # vLLM server configuration
  vllm:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    dtype: auto
    max_model_len: 32768
    port: 8000
    trust_remote_code: true

benchmarks:
  bfcl:
    use_native_tools: false
  ko_mtbench:
    temperature: 0.7
