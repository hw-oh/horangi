# =============================================================================
# Grok-4-1-Fast-Reasoning Model Configuration
# =============================================================================

# Model identifier - using LiteLLM native xAI support
# Format: xai/<model-name> (LiteLLM handles API endpoint automatically)
model_id: litellm/xai/grok-4-1-fast-reasoning

# Model metadata
metadata:
  provider: xai
  name: grok-4-1-fast-reasoning
  release_date: "2025-11-17"
  size_category: None
  model_size: None
  context_window: 2000000
  max_output_tokens: 65536

# API settings
api_key_env: XAI_API_KEY

# Default parameters (merged with base_config defaults)
defaults:
  temperature: 1.0
  max_tokens: 65536

# Benchmark-specific overrides
benchmarks:
  # BFCL: Native tool calling supported
  bfcl:
    use_native_tools: true
  
  # SWE-bench: Requires long context
  swebench_verified_official_80:
    max_tokens: 65536
