"""
í‰ê°€ ê²°ê³¼ë¥¼ ë¦¬ë”ë³´ë“œ í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ Weaveì— ì €ì¥ëœ Inspect AI í‰ê°€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ
pandas DataFrame í˜•íƒœì˜ ë¦¬ë”ë³´ë“œ í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    from core.leaderboard_table import LeaderboardTableBuilder
    
    # ë¹Œë” ì´ˆê¸°í™”
    builder = LeaderboardTableBuilder(
        entity="wandb-korea",
        project="evaluation-job",
        model_name="gpt-4o",
        release_date="2024-05-13",
        size_category="flagship",
        model_size="unknown",
    )
    
    # Weave traceì—ì„œ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìˆ˜ì§‘
    builder.collect_from_weave_traces()
    
    # ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ê²°ê³¼ ì¶”ê°€
    builder.add_benchmark_result("ko_hle", {"score": 0.85})
    
    # ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„± ë° ë¡œê¹…
    builder.build_and_log()
"""

from __future__ import annotations

from typing import Any, Optional
from dataclasses import dataclass, field
import pandas as pd

try:
    import wandb
    import weave
except ImportError:
    wandb = None
    weave = None


# =============================================================================
# ë²¤ì¹˜ë§ˆí¬-ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì„¤ì •
# =============================================================================

# ê° ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‚¬ìš©í•  ì ìˆ˜ ì»¬ëŸ¼ê³¼ GLP/ALT ë§¤í•‘
BENCHMARK_CONFIG = {
    # MT-Bench: ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜
    "mtbench_ko": {
        "columns": ["model_name", "roleplay", "humanities", "writing", "reasoning", "coding", "math", "stem", "extraction"],
        "mapper": {
            "roleplay": "GLP_í‘œí˜„",
            "humanities": "GLP_í‘œí˜„",
            "writing": "GLP_í‘œí˜„",
            "reasoning": "GLP_ë…¼ë¦¬ì ì¶”ë¡ ",
            "coding": "GLP_ì½”ë”©ëŠ¥ë ¥",
            "math": "GLP_ìˆ˜í•™ì ì¶”ë¡ ",
            "stem": "GLP_ì „ë¬¸ì ì§€ì‹",
            "extraction": "GLP_ì •ë³´ê²€ìƒ‰",
        },
        "score_key": "avg_score",  # ì „ì²´ í‰ê·  ì ìˆ˜
    },
    # HLE: ì „ë¬¸ì  ì§€ì‹
    "ko_hle": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ì „ë¬¸ì ì§€ì‹"},
        "score_key": "accuracy",  # Inspect AIì˜ ê¸°ë³¸ ì ìˆ˜ í‚¤
    },
    # AIME2025: ìˆ˜í•™ì  ì¶”ë¡ 
    "ko_aime2025": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ìˆ˜í•™ì ì¶”ë¡ "},
        "score_key": "accuracy",
    },
    # GSM8K: ìˆ˜í•™ì  ì¶”ë¡ 
    "ko_gsm8k": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ìˆ˜í•™ì ì¶”ë¡ "},
        "score_key": "accuracy",
    },
    # Ko-BALT-700
    "ko_balt_700_syntax": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_êµ¬ë¬¸í•´ì„"},  # ê¸°ë³¸ ë§¤í•‘
        "score_key": "accuracy",
    },
    "ko_balt_700_semantic": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ì˜ë¯¸í•´ì„"},  # ê¸°ë³¸ ë§¤í•‘
        "score_key": "accuracy",
    },
    # KMMLU: ì¼ë°˜ì  ì§€ì‹
    "kmmlu": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ì¼ë°˜ì ì§€ì‹"},
        "score_key": "accuracy",
    },
    # KMMLU Pro: ì „ë¬¸ì  ì§€ì‹
    "kmmlu_pro": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ì „ë¬¸ì ì§€ì‹"},
        "score_key": "accuracy",
    },
    # Korean Hate Speech: ìœ í•´ì„± ë°©ì§€
    "korean_hate_speech": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_ìœ í•´ì„±ë°©ì§€"},
        "score_key": "macro_f1",  # F1 ì ìˆ˜ ì‚¬ìš©
    },
    # HAERAE Bench V1 w/ RC: ì˜ë¯¸í•´ì„
    "haerae_bench_v1_rc": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ì˜ë¯¸í•´ì„"},
        "score_key": "accuracy",
    },
    # HAERAE Bench V1 w/o RC: ì¼ë°˜ì  ì§€ì‹
    "haerae_bench_v1_wo_rc": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ì¼ë°˜ì ì§€ì‹"},
        "score_key": "accuracy",
    },
    # IFEval-Ko: ì œì–´ì„±
    "ifeval_ko": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_ì œì–´ì„±"},
        "score_key": "accuracy",
    },
    # Squad-Kor-V1: ì •ë³´ê²€ìƒ‰
    "squad_kor_v1": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ì •ë³´ê²€ìƒ‰"},
        "score_key": "accuracy",
    },
    # KoBBQ: í¸í–¥ì„± ë°©ì§€
    "kobbq": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_í¸í–¥ì„±ë°©ì§€"},
        "score_key": "accuracy",
    },
    # Ko-Moral: ìœ¤ë¦¬/ë„ë•
    "ko_moral": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_ìœ¤ë¦¬/ë„ë•"},
        "score_key": "accuracy",
    },
    # Ko-TruthfulQA: í™˜ê° ë°©ì§€ ê´€ë ¨
    "ko_truthful_qa": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_í™˜ê°ë°©ì§€"},
        "score_key": "accuracy",
    },
    # Ko-ARC-AGI: ì¶”ìƒì  ì¶”ë¡ 
    "ko_arc_agi": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ì¶”ìƒì ì¶”ë¡ "},
        "score_key": "accuracy",
    },
    # SWE-bench: ì½”ë”© ëŠ¥ë ¥
    "swebench_verified_official_80": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_ì½”ë”©ëŠ¥ë ¥"},
        "score_key": "resolved",  # SWE-benchì˜ í•´ê²°ë¥ 
    },
    # BFCL: í•¨ìˆ˜ í˜¸ì¶œ
    "bfcl": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_í•¨ìˆ˜í˜¸ì¶œ"},
        "score_key": "accuracy",
    },
    # Ko-HellaSwag: ìƒì‹ ì¶”ë¡  (ê¸°ë³¸ ì–¸ì–´ ì„±ëŠ¥)
    "ko_hellaswag": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "GLP_êµ¬ë¬¸í•´ì„"},
        "score_key": "accuracy",
    },
    # HalluLens ë²¤ì¹˜ë§ˆí¬ë“¤: í™˜ê° ë°©ì§€
    "ko_hallulens_wikiqa": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_í™˜ê°ë°©ì§€"},
        "score_key": "accuracy",
    },
    "ko_hallulens_longwiki": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_í™˜ê°ë°©ì§€"},
        "score_key": "accuracy",
    },
    "ko_hallulens_generated": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_í™˜ê°ë°©ì§€"},
        "score_key": "refusal_rate",
    },
    "ko_hallulens_mixed": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_í™˜ê°ë°©ì§€"},
        "score_key": "accuracy",
    },
    "ko_hallulens_nonexistent": {
        "columns": ["model_name", "score"],
        "mapper": {"score": "ALT_í™˜ê°ë°©ì§€"},
        "score_key": "accuracy",
    },
}

# GLP (ë²”ìš©ì–¸ì–´ì„±ëŠ¥) ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
GLP_COLUMN_WEIGHT = {
    "GLP_êµ¬ë¬¸í•´ì„": 1,
    "GLP_ì˜ë¯¸í•´ì„": 1,
    "GLP_í‘œí˜„": 1,
    "GLP_ë²ˆì—­": 1,
    "GLP_ì •ë³´ê²€ìƒ‰": 1,
    "GLP_ì¼ë°˜ì ì§€ì‹": 2,
    "GLP_ì „ë¬¸ì ì§€ì‹": 2,
    "GLP_ìˆ˜í•™ì ì¶”ë¡ ": 2,
    "GLP_ë…¼ë¦¬ì ì¶”ë¡ ": 2,
    "GLP_ì¶”ìƒì ì¶”ë¡ ": 2,
    "GLP_í•¨ìˆ˜í˜¸ì¶œ": 2,
    "GLP_ì½”ë”©ëŠ¥ë ¥": 2,
}

# ALT (ê°€ì¹˜ì •ë ¬ì„±ëŠ¥) ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
ALT_COLUMN_WEIGHT = {
    "ALT_ì œì–´ì„±": 1,
    "ALT_ìœ í•´ì„±ë°©ì§€": 1,
    "ALT_í¸í–¥ì„±ë°©ì§€": 1,
    "ALT_ìœ¤ë¦¬/ë„ë•": 1,
    "ALT_í™˜ê°ë°©ì§€": 1,
}

# GLP ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ â†’ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë ˆì´ë” ì°¨íŠ¸ìš©)
GLP_COLUMN_MAPPER = {
    "GLP_êµ¬ë¬¸í•´ì„": "ê¸°ë³¸ì–¸ì–´ì„±ëŠ¥",
    "GLP_ì˜ë¯¸í•´ì„": "ê¸°ë³¸ì–¸ì–´ì„±ëŠ¥",
    "GLP_í‘œí˜„": "ì‘ìš©ì–¸ì–´ì„±ëŠ¥",
    "GLP_ë²ˆì—­": "ì‘ìš©ì–¸ì–´ì„±ëŠ¥",
    "GLP_ì •ë³´ê²€ìƒ‰": "ì‘ìš©ì–¸ì–´ì„±ëŠ¥",
    "GLP_ì¼ë°˜ì ì§€ì‹": "ì§€ì‹/ì§ˆì˜ì‘ë‹µ",
    "GLP_ì „ë¬¸ì ì§€ì‹": "ì§€ì‹/ì§ˆì˜ì‘ë‹µ",
    "GLP_ìˆ˜í•™ì ì¶”ë¡ ": "ì¶”ë¡ ëŠ¥ë ¥",
    "GLP_ë…¼ë¦¬ì ì¶”ë¡ ": "ì¶”ë¡ ëŠ¥ë ¥",
    "GLP_ì¶”ìƒì ì¶”ë¡ ": "ì¶”ë¡ ëŠ¥ë ¥",
    "GLP_í•¨ìˆ˜í˜¸ì¶œ": "ì–´í”Œë¦¬ì¼€ì´ì…˜ê°œë°œ",
    "GLP_ì½”ë”©ëŠ¥ë ¥": "ì–´í”Œë¦¬ì¼€ì´ì…˜ê°œë°œ",
}

# ALT ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ â†’ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë ˆì´ë” ì°¨íŠ¸ìš©)
ALT_COLUMN_MAPPER = {
    "ALT_ì œì–´ì„±": "ì œì–´ì„±",
    "ALT_ìœ í•´ì„±ë°©ì§€": "ìœ í•´ì„±ë°©ì§€",
    "ALT_í¸í–¥ì„±ë°©ì§€": "í¸í–¥ì„±ë°©ì§€",
    "ALT_ìœ¤ë¦¬/ë„ë•": "ìœ¤ë¦¬/ë„ë•",
    "ALT_í™˜ê°ë°©ì§€": "í™˜ê°ë°©ì§€",
}


# =============================================================================
# í—¬í¼ í•¨ìˆ˜
# =============================================================================

def weighted_average(df: pd.DataFrame, weights_dict: dict[str, float]) -> pd.Series:
    """ê°€ì¤‘ í‰ê·  ê³„ì‚°"""
    cols = [c for c in weights_dict.keys() if c in df.columns]
    if not cols:
        return pd.Series([float('nan')] * len(df))
    weights = [weights_dict[c] for c in cols]
    return (df[cols].mul(weights, axis=1).sum(axis=1)) / sum(weights)


def extract_score_from_results(results: dict, score_key: str) -> float | None:
    """
    Inspect AI ê²°ê³¼ì—ì„œ ì ìˆ˜ ì¶”ì¶œ
    
    results êµ¬ì¡° ì˜ˆì‹œ:
    {
        "scores": [{"name": "accuracy", "metrics": {"accuracy": {"value": 0.85}}}],
        ...
    }
    """
    if not results:
        return None
    
    # scores ë°°ì—´ì—ì„œ ì ìˆ˜ ì°¾ê¸°
    scores = results.get("scores", [])
    for score in scores:
        metrics = score.get("metrics", {})
        if score_key in metrics:
            metric = metrics[score_key]
            if isinstance(metric, dict):
                return metric.get("value")
            return metric
        # nameìœ¼ë¡œ ì°¾ê¸°
        if score.get("name") == score_key:
            for metric_name, metric_value in metrics.items():
                if isinstance(metric_value, dict):
                    return metric_value.get("value")
                return metric_value
    
    # ì§ì ‘ í‚¤ë¡œ ì ‘ê·¼ ì‹œë„
    if score_key in results:
        val = results[score_key]
        if isinstance(val, dict):
            return val.get("value", val.get("mean"))
        return val
    
    return None


# =============================================================================
# LeaderboardTableBuilder
# =============================================================================

@dataclass
class LeaderboardTableBuilder:
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ë¦¬ë”ë³´ë“œ í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ëŠ” ë¹Œë”
    
    Attributes:
        entity: W&B/Weave entity (íŒ€ ë˜ëŠ” ì‚¬ìš©ì ì´ë¦„)
        project: W&B/Weave í”„ë¡œì íŠ¸ ì´ë¦„
        model_name: í‰ê°€ ëŒ€ìƒ ëª¨ë¸ ì´ë¦„
        release_date: ëª¨ë¸ ì¶œì‹œì¼ (YYYY-MM-DD)
        size_category: ëª¨ë¸ í¬ê¸° ì¹´í…Œê³ ë¦¬ (small, medium, large, flagship ë“±)
        model_size: ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ (ì˜ˆ: "7B", "13B", "70B")
    """
    entity: str
    project: str
    model_name: str
    release_date: str = "unknown"
    size_category: str = "unknown"
    model_size: str = "unknown"
    
    # ë‚´ë¶€ ìƒíƒœ
    benchmark_results: dict[str, dict] = field(default_factory=dict)
    _wandb_run: Any = field(default=None)
    
    def add_benchmark_result(
        self,
        benchmark_name: str,
        scores: dict[str, float],
    ) -> None:
        """
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶”ê°€
        
        Args:
            benchmark_name: ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ (ì˜ˆ: "ko_hle", "mtbench_ko")
            scores: ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ (ì˜ˆ: {"accuracy": 0.85} ë˜ëŠ” 
                    {"roleplay": 8.5, "writing": 7.8, ...} for mtbench)
        """
        self.benchmark_results[benchmark_name] = scores
    
    def collect_from_weave_traces(
        self,
        model_filter: str | None = None,
        benchmark_filter: list[str] | None = None,
        limit: int = 100,
    ) -> None:
        """
        Weave traceì—ì„œ í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘
        
        Args:
            model_filter: íŠ¹ì • ëª¨ë¸ë§Œ í•„í„°ë§ (ì—†ìœ¼ë©´ self.model_name ì‚¬ìš©)
            benchmark_filter: íŠ¹ì • ë²¤ì¹˜ë§ˆí¬ë§Œ ìˆ˜ì§‘ (ì—†ìœ¼ë©´ ì „ì²´)
            limit: ìµœëŒ€ trace ìˆ˜
        """
        if weave is None:
            raise ImportError("weave íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # Weave ì´ˆê¸°í™”
        weave.init(f"{self.entity}/{self.project}")
        client = weave.get_client()
        
        if client is None:
            raise RuntimeError("Weave í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        target_model = model_filter or self.model_name
        
        try:
            # Inspect AI í‰ê°€ trace ê²€ìƒ‰
            # op_nameì´ "inspect_ai" ë˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ì„ í¬í•¨í•˜ëŠ” ê²ƒë“¤
            calls = client.calls(
                filter={
                    "trace_roots_only": True,  # ìµœìƒìœ„ traceë§Œ
                },
                limit=limit,
            )
            
            for call in calls:
                # ëª¨ë¸ í•„í„°ë§
                call_model = self._extract_model_from_call(call)
                if target_model and call_model and target_model not in call_model:
                    continue
                
                # ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ ì¶”ì¶œ
                benchmark_name = self._extract_benchmark_from_call(call)
                if not benchmark_name:
                    continue
                
                # ë²¤ì¹˜ë§ˆí¬ í•„í„°ë§
                if benchmark_filter and benchmark_name not in benchmark_filter:
                    continue
                
                # ì ìˆ˜ ì¶”ì¶œ
                scores = self._extract_scores_from_call(call, benchmark_name)
                if scores:
                    self.benchmark_results[benchmark_name] = scores
                    print(f"  âœ“ {benchmark_name}: {scores}")
        
        except Exception as e:
            print(f"âš ï¸ Weave trace ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    def _extract_model_from_call(self, call) -> str | None:
        """Callì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ"""
        try:
            if hasattr(call, "inputs") and call.inputs:
                return call.inputs.get("model")
            if hasattr(call, "attributes") and call.attributes:
                return call.attributes.get("model")
        except Exception:
            pass
        return None
    
    def _extract_benchmark_from_call(self, call) -> str | None:
        """Callì—ì„œ ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ ì¶”ì¶œ"""
        try:
            # op_nameì—ì„œ ì¶”ì¶œ
            op_name = getattr(call, "op_name", "") or ""
            for benchmark in BENCHMARK_CONFIG.keys():
                if benchmark in op_name.lower():
                    return benchmark
            
            # attributesì—ì„œ ì¶”ì¶œ
            if hasattr(call, "attributes") and call.attributes:
                task = call.attributes.get("task") or call.attributes.get("benchmark")
                if task and task in BENCHMARK_CONFIG:
                    return task
            
            # inputsì—ì„œ ì¶”ì¶œ
            if hasattr(call, "inputs") and call.inputs:
                task = call.inputs.get("task") or call.inputs.get("benchmark")
                if task and task in BENCHMARK_CONFIG:
                    return task
        except Exception:
            pass
        return None
    
    def _extract_scores_from_call(self, call, benchmark_name: str) -> dict | None:
        """Callì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        try:
            output = call.output if hasattr(call, "output") else None
            if not output:
                return None
            
            config = BENCHMARK_CONFIG.get(benchmark_name, {})
            score_key = config.get("score_key", "accuracy")
            
            # outputì´ dictì¸ ê²½ìš°
            if isinstance(output, dict):
                score = extract_score_from_results(output, score_key)
                if score is not None:
                    return {"score": score}
            
            # ì§ì ‘ ê°’ì¸ ê²½ìš°
            if isinstance(output, (int, float)):
                return {"score": output}
        
        except Exception:
            pass
        return None
    
    def build_leaderboard_df(self) -> pd.DataFrame:
        """
        ìˆ˜ì§‘ëœ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¡œ ë¦¬ë”ë³´ë“œ DataFrame ìƒì„±
        
        Returns:
            ë¦¬ë”ë³´ë“œ DataFrame (GLP/ALT ì ìˆ˜ í¬í•¨)
        """
        if not self.benchmark_results:
            raise ValueError("ìˆ˜ì§‘ëœ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì´ˆê¸° DataFrame ìƒì„±
        data = {"model_name": [self.model_name]}
        
        # ê° ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ GLP/ALT ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘
        for benchmark_name, scores in self.benchmark_results.items():
            config = BENCHMARK_CONFIG.get(benchmark_name, {})
            mapper = config.get("mapper", {})
            
            # ë‹¨ì¼ scoreì¸ ê²½ìš°
            if "score" in scores and len(mapper) == 1:
                category = list(mapper.values())[0]
                if category not in data:
                    data[category] = []
                # ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ê°€ ê°™ì€ ì¹´í…Œê³ ë¦¬ì— ë§¤í•‘ë˜ë©´ í‰ê·  ê³„ì‚°ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
                if len(data[category]) < 1:
                    data[category].append(scores["score"])
                else:
                    # ì´ë¯¸ ê°’ì´ ìˆìœ¼ë©´ í‰ê·  ê³„ì‚°
                    data[category][0] = (data[category][0] + scores["score"]) / 2
            
            # ì—¬ëŸ¬ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš° (mtbench ë“±)
            else:
                for score_field, category in mapper.items():
                    if score_field in scores:
                        if category not in data:
                            data[category] = []
                        if len(data[category]) < 1:
                            data[category].append(scores[score_field])
                        else:
                            data[category][0] = (data[category][0] + scores[score_field]) / 2
        
        # ëª¨ë“  ê°’ì´ ê°™ì€ ê¸¸ì´ì¸ì§€ í™•ì¸
        max_len = max(len(v) if isinstance(v, list) else 1 for v in data.values())
        for key in data:
            if isinstance(data[key], list) and len(data[key]) < max_len:
                data[key].extend([float('nan')] * (max_len - len(data[key])))
        
        df = pd.DataFrame(data)
        
        # GLP/ALT í‰ê·  ê³„ì‚°
        df['ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG'] = weighted_average(df, GLP_COLUMN_WEIGHT)
        df['ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG'] = weighted_average(df, ALT_COLUMN_WEIGHT)
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        glp_score = df['ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG'].iloc[0] if 'ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG' in df.columns else float('nan')
        alt_score = df['ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG'].iloc[0] if 'ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG' in df.columns else float('nan')
        
        if pd.notna(glp_score) and pd.notna(alt_score):
            df['FINAL_SCORE'] = (glp_score + alt_score) / 2
        elif pd.notna(glp_score):
            df['FINAL_SCORE'] = glp_score
        elif pd.notna(alt_score):
            df['FINAL_SCORE'] = alt_score
        else:
            df['FINAL_SCORE'] = float('nan')
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        df['release_date'] = pd.to_datetime(self.release_date, format='%Y-%m-%d', errors='coerce')
        df['size_category'] = self.size_category
        df['model_size'] = self.model_size
        
        # ì»¬ëŸ¼ ì •ë ¬
        desired_columns = [
            'model_name', 'release_date', 'size_category', 'model_size', 
            'FINAL_SCORE', 'ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG', 'ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG'
        ] + list(GLP_COLUMN_WEIGHT.keys()) + list(ALT_COLUMN_WEIGHT.keys())
        
        existing_columns = [col for col in desired_columns if col in df.columns]
        return df[existing_columns]
    
    def build_radar_tables(
        self,
        df: pd.DataFrame | None = None,
    ) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        ë ˆì´ë” ì°¨íŠ¸ìš© í…Œì´ë¸” ìƒì„±
        
        Args:
            df: ë¦¬ë”ë³´ë“œ DataFrame (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
        
        Returns:
            (glp_radar_table, glp_detail_radar_table, 
             alt_radar_table, alt_detail_radar_table)
        """
        if df is None:
            df = self.build_leaderboard_df()
        
        # GLP ë ˆì´ë” í…Œì´ë¸”
        glp_cols = [c for c in GLP_COLUMN_MAPPER.keys() if c in df.columns]
        if glp_cols:
            glp_radar_table = (
                df[glp_cols]
                .rename(columns=GLP_COLUMN_MAPPER)
                .transpose()
                .reset_index()
                .groupby("index")
                .mean()
                .reset_index()
                .rename(columns={'index': 'category', 0: 'score'})
            )
            glp_detail_radar_table = (
                df[glp_cols]
                .transpose()
                .reset_index()
                .rename(columns={'index': 'category', 0: 'score'})
            )
        else:
            glp_radar_table = pd.DataFrame(columns=['category', 'score'])
            glp_detail_radar_table = pd.DataFrame(columns=['category', 'score'])
        
        # ALT ë ˆì´ë” í…Œì´ë¸”
        alt_cols = [c for c in ALT_COLUMN_MAPPER.keys() if c in df.columns]
        if alt_cols:
            alt_radar_table = (
                df[alt_cols]
                .rename(columns=ALT_COLUMN_MAPPER)
                .transpose()
                .reset_index()
                .groupby("index")
                .mean()
                .reset_index()
                .rename(columns={'index': 'category', 0: 'score'})
            )
            alt_detail_radar_table = (
                df[alt_cols]
                .transpose()
                .reset_index()
                .rename(columns={'index': 'category', 0: 'score'})
            )
        else:
            alt_radar_table = pd.DataFrame(columns=['category', 'score'])
            alt_detail_radar_table = pd.DataFrame(columns=['category', 'score'])
        
        return glp_radar_table, glp_detail_radar_table, alt_radar_table, alt_detail_radar_table
    
    def build_and_log(
        self,
        wandb_project: str | None = None,
        log_radar_tables: bool = True,
    ) -> pd.DataFrame:
        """
        ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„± ë° W&Bì— ë¡œê¹…
        
        Args:
            wandb_project: W&B í”„ë¡œì íŠ¸ (ì—†ìœ¼ë©´ self.project ì‚¬ìš©)
            log_radar_tables: ë ˆì´ë” í…Œì´ë¸”ë„ ë¡œê¹…í• ì§€ ì—¬ë¶€
        
        Returns:
            ë¦¬ë”ë³´ë“œ DataFrame
        """
        if wandb is None:
            raise ImportError("wandb íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„±
        leaderboard_df = self.build_leaderboard_df()
        
        # W&B ì´ˆê¸°í™” (í•„ìš”í•œ ê²½ìš°)
        if self._wandb_run is None:
            self._wandb_run = wandb.init(
                project=wandb_project or self.project,
                entity=self.entity,
                job_type="leaderboard",
                name=f"leaderboard-{self.model_name}",
            )
        
        # ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ë¡œê¹…
        leaderboard_table = wandb.Table(dataframe=leaderboard_df)
        wandb.log({"leaderboard_table": leaderboard_table})
        
        # ë ˆì´ë” í…Œì´ë¸” ë¡œê¹…
        if log_radar_tables:
            glp_radar, glp_detail, alt_radar, alt_detail = self.build_radar_tables(leaderboard_df)
            
            wandb.log({
                "glp_radar_table": wandb.Table(dataframe=glp_radar),
                "glp_detail_radar_table": wandb.Table(dataframe=glp_detail),
                "alt_radar_table": wandb.Table(dataframe=alt_radar),
                "alt_detail_radar_table": wandb.Table(dataframe=alt_detail),
            })
        
        print(f"âœ… ë¦¬ë”ë³´ë“œ í…Œì´ë¸”ì´ W&Bì— ë¡œê¹…ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   í”„ë¡œì íŠ¸: {self.entity}/{wandb_project or self.project}")
        
        return leaderboard_df
    
    def finish(self) -> None:
        """W&B run ì¢…ë£Œ"""
        if self._wandb_run is not None:
            self._wandb_run.finish()
            self._wandb_run = None


# =============================================================================
# í¸ì˜ í•¨ìˆ˜
# =============================================================================

def create_leaderboard_from_benchmarks(
    entity: str,
    project: str,
    model_name: str,
    benchmark_results: dict[str, dict[str, float]],
    release_date: str = "unknown",
    size_category: str = "unknown",
    model_size: str = "unknown",
    log_to_wandb: bool = True,
) -> pd.DataFrame:
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¡œ ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„± (í¸ì˜ í•¨ìˆ˜)
    
    Args:
        entity: W&B entity
        project: W&B í”„ë¡œì íŠ¸
        model_name: ëª¨ë¸ ì´ë¦„
        benchmark_results: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            ì˜ˆ: {"ko_hle": {"score": 0.85}, "kmmlu": {"score": 0.72}}
        release_date: ëª¨ë¸ ì¶œì‹œì¼
        size_category: ëª¨ë¸ í¬ê¸° ì¹´í…Œê³ ë¦¬
        model_size: ëª¨ë¸ í¬ê¸°
        log_to_wandb: W&Bì— ë¡œê¹…í• ì§€ ì—¬ë¶€
    
    Returns:
        ë¦¬ë”ë³´ë“œ DataFrame
    
    Example:
        >>> df = create_leaderboard_from_benchmarks(
        ...     entity="my-team",
        ...     project="korean-llm-eval",
        ...     model_name="gpt-4o",
        ...     benchmark_results={
        ...         "ko_hle": {"score": 0.42},
        ...         "kmmlu": {"score": 0.78},
        ...         "kmmlu_pro": {"score": 0.65},
        ...         "kobbq": {"score": 0.82},
        ...         "korean_hate_speech": {"score": 0.91},
        ...     },
        ...     release_date="2024-05-13",
        ...     size_category="flagship",
        ... )
    """
    builder = LeaderboardTableBuilder(
        entity=entity,
        project=project,
        model_name=model_name,
        release_date=release_date,
        size_category=size_category,
        model_size=model_size,
    )
    
    for benchmark_name, scores in benchmark_results.items():
        builder.add_benchmark_result(benchmark_name, scores)
    
    if log_to_wandb:
        return builder.build_and_log()
    else:
        return builder.build_leaderboard_df()


def aggregate_multiple_models(
    model_results: list[pd.DataFrame],
) -> pd.DataFrame:
    """
    ì—¬ëŸ¬ ëª¨ë¸ì˜ ë¦¬ë”ë³´ë“œ í…Œì´ë¸” í†µí•©
    
    Args:
        model_results: ê° ëª¨ë¸ì˜ ë¦¬ë”ë³´ë“œ DataFrame ë¦¬ìŠ¤íŠ¸
    
    Returns:
        í†µí•©ëœ ë¦¬ë”ë³´ë“œ DataFrame
    """
    if not model_results:
        return pd.DataFrame()
    
    return pd.concat(model_results, ignore_index=True)


# =============================================================================
# CLI ì§€ì›
# =============================================================================

def main():
    """CLI ì§„ì…ì """
    import argparse
    
    parser = argparse.ArgumentParser(
        description="í‰ê°€ ê²°ê³¼ë¡œ ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„±"
    )
    parser.add_argument(
        "--entity", "-e",
        required=True,
        help="W&B entity (íŒ€ ë˜ëŠ” ì‚¬ìš©ì ì´ë¦„)"
    )
    parser.add_argument(
        "--project", "-p",
        required=True,
        help="W&B í”„ë¡œì íŠ¸ ì´ë¦„"
    )
    parser.add_argument(
        "--model", "-m",
        required=True,
        help="ëª¨ë¸ ì´ë¦„"
    )
    parser.add_argument(
        "--release-date",
        default="unknown",
        help="ëª¨ë¸ ì¶œì‹œì¼ (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--size-category",
        default="unknown",
        help="ëª¨ë¸ í¬ê¸° ì¹´í…Œê³ ë¦¬"
    )
    parser.add_argument(
        "--model-size",
        default="unknown",
        help="ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜"
    )
    parser.add_argument(
        "--from-weave",
        action="store_true",
        help="Weave traceì—ì„œ ê²°ê³¼ ìˆ˜ì§‘"
    )
    parser.add_argument(
        "--output", "-o",
        help="ê²°ê³¼ë¥¼ ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ"
    )
    
    args = parser.parse_args()
    
    builder = LeaderboardTableBuilder(
        entity=args.entity,
        project=args.project,
        model_name=args.model,
        release_date=args.release_date,
        size_category=args.size_category,
        model_size=args.model_size,
    )
    
    if args.from_weave:
        print(f"ğŸ” Weave traceì—ì„œ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...")
        builder.collect_from_weave_traces()
    
    if builder.benchmark_results:
        df = builder.build_and_log()
        
        if args.output:
            df.to_csv(args.output, index=False)
            print(f"ğŸ“ ê²°ê³¼ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print("\nğŸ“Š ë¦¬ë”ë³´ë“œ í…Œì´ë¸”:")
        print(df.to_string())
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

