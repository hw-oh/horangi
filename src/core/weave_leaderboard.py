"""
Weave Leaderboard ìë™ ìƒì„± ëª¨ë“ˆ

Inspect AI í‰ê°€ ê²°ê³¼ì—ì„œ Weave Leaderboardë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
ì´ ë¦¬ë”ë³´ë“œëŠ” Weave UIì—ì„œ ëª¨ë¸ ê°„ ì„±ëŠ¥ ë¹„êµë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # run_eval.pyì—ì„œ ìë™ í˜¸ì¶œë¨
    from core.weave_leaderboard import create_weave_leaderboard
    
    create_weave_leaderboard(
        entity="wandb-korea",
        project="korean-llm-eval",
        model_name="gpt-4o",
    )

Note:
    - W&B Models í…Œì´ë¸” (leaderboard_table.py)ê³¼ëŠ” ë³„ê°œë¡œ ì‘ë™í•©ë‹ˆë‹¤.
    - Weave UIì˜ Leaderboard ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import weave
from weave.flow import leaderboard
from weave.trace import urls as weave_urls


# ë¦¬ë”ë³´ë“œ ì„¤ì •
LEADERBOARD_REF = "Korean-LLM-Leaderboard"
LEADERBOARD_NAME = "Korean LLM Leaderboard"
LEADERBOARD_DESCRIPTION = """í•œêµ­ì–´ LLM ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬ë”ë³´ë“œ

ì´ ë¦¬ë”ë³´ë“œëŠ” Inspect AI í‰ê°€ ê²°ê³¼ì—ì„œ ìë™ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ë¹„êµí•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì¹´í…Œê³ ë¦¬:
- ì–¸ì–´ ì´í•´: ko_hellaswag, kmmlu, kmmlu_pro, haerae_bench
- ì¶”ë¡ : ko_aime2025, ko_gsm8k, ko_arc_agi
- ì§€ì‹œ ë”°ë¥´ê¸°: ifeval_ko, ko_balt_700
- ì•ˆì „ì„±/ìœ¤ë¦¬: ko_moral, kobbq, korean_hate_speech
- í™˜ê°: ko_hallulens (wikiqa, longwiki, nonexistent)
- ì§€ì‹: ko_truthful_qa, ko_hle
- ë„êµ¬ ì‚¬ìš©: bfcl
- ëŒ€í™”: mtbench_ko
- ì½”ë”©: swebench_verified_official_80
"""


def get_evaluation_ref(entity: str, project: str, benchmark: str) -> str | None:
    """
    ë²¤ì¹˜ë§ˆí¬ì— í•´ë‹¹í•˜ëŠ” evaluation ê°ì²´ì˜ ì‹¤ì œ refë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    :latest íƒœê·¸ëŠ” Leaderboardì—ì„œ ì‘ë™í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
    ì‹¤ì œ digestê°€ í¬í•¨ëœ refë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    from weave.trace.ref_util import get_ref
    
    try:
        eval_name = f"{benchmark}-evaluation"
        eval_obj = weave.ref(f"{eval_name}:latest").get()
        ref = get_ref(eval_obj)
        if ref:
            return ref.uri()
    except Exception:
        pass
    
    return None


def build_columns_from_benchmarks(
    benchmarks: list[str],
    entity: str,
    project: str,
) -> list[leaderboard.LeaderboardColumn]:
    """
    ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ ëª©ë¡ì—ì„œ LeaderboardColumn ìƒì„±
    
    ê° ë²¤ì¹˜ë§ˆí¬ì˜ evaluation refë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì™€ ì»¬ëŸ¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        benchmarks: ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        entity: Weave entity
        project: Weave í”„ë¡œì íŠ¸ ì´ë¦„
    
    Returns:
        LeaderboardColumn ë¦¬ìŠ¤íŠ¸
    """
    # ë²¤ì¹˜ë§ˆí¬ë³„ ì£¼ìš” ë©”íŠ¸ë¦­ ë§¤í•‘
    # (scorer_name, summary_metric_path) í˜•íƒœ
    # output êµ¬ì¡°: {"scorer_name": {"metric": value, ...}, ...}
    BENCHMARK_METRICS = {
        # ê¸°ë³¸ choice scorer
        "ko_hellaswag": ("choice", "true_fraction"),
        "ko_balt_700_syntax": ("choice", "true_fraction"),
        "ko_balt_700_semantic": ("choice", "true_fraction"),
        "haerae_bench_v1_rc": ("choice", "true_fraction"),
        "haerae_bench_v1_wo_rc": ("choice", "true_fraction"),
        "kmmlu": ("choice", "true_fraction"),
        "kmmlu_pro": ("choice", "true_fraction"),
        "ko_truthful_qa": ("choice", "true_fraction"),
        "ko_moral": ("choice", "true_fraction"),
        "korean_hate_speech": ("choice", "true_fraction"),
        
        # model_graded_qa scorer
        "ko_aime2025": ("model_graded_qa", "true_fraction"),
        "ko_gsm8k": ("model_graded_qa", "true_fraction"),
        
        # íŠ¹ìˆ˜ scorer
        "ifeval_ko": ("instruction_following", "prompt_level_strict.true_fraction"),
        "ko_arc_agi": ("grid_match", "true_fraction"),
        "squad_kor_v1": ("f1", "mean"),
        
        # KoBBQ
        "kobbq": ("kobbq_scorer", "true_fraction"),
        
        # HLE
        "ko_hle": ("hle_grader", "true_fraction"),
        
        # HalluLens
        "ko_hallulens_wikiqa": ("hallulens_qa", "true_fraction"),
        "ko_hallulens_longwiki": ("hallulens_qa", "true_fraction"),
        "ko_hallulens_nonexistent": ("hallulens_refusal", "true_fraction"),
        
        # BFCL
        "bfcl": ("bfcl_scorer", "true_fraction"),
        
        # MT-Bench
        "mtbench_ko": ("mtbench_scorer", "mean"),
        
        # SWE-bench
        "swebench_verified_official_80": ("swebench_server_scorer", "true_fraction"),
    }
    
    columns = []
    
    for benchmark in benchmarks:
        # ì‹¤ì œ evaluation ref ê°€ì ¸ì˜¤ê¸° (digest í¬í•¨)
        eval_ref = get_evaluation_ref(entity, project, benchmark)
        
        if not eval_ref:
            print(f"   âš ï¸ {benchmark}-evaluation ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            continue
        
        # í•´ë‹¹ ë²¤ì¹˜ë§ˆí¬ì˜ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
        scorer_name, metric_path = BENCHMARK_METRICS.get(
            benchmark, ("output", "true_fraction")
        )
        
        columns.append(
            leaderboard.LeaderboardColumn(
                evaluation_object_ref=eval_ref,
                scorer_name=scorer_name,
                summary_metric_path=metric_path,
                should_minimize=False,
            )
        )
        print(f"   âœ“ {benchmark}: {scorer_name}.{metric_path}")
    
    return columns


def create_weave_leaderboard(
    entity: str,
    project: str,
    benchmarks: list[str] | None = None,
    name: str = LEADERBOARD_NAME,
    description: str = LEADERBOARD_DESCRIPTION,
) -> str | None:
    """
    Weave Leaderboard ìƒì„±/ì—…ë°ì´íŠ¸
    
    ë²¤ì¹˜ë§ˆí¬ ëª©ë¡ì„ ë°›ì•„ì„œ Weave Leaderboardë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ê¸°ì¡´ ë¦¬ë”ë³´ë“œê°€ ìˆìœ¼ë©´ ìƒˆ ì»¬ëŸ¼ì„ ë³‘í•©í•©ë‹ˆë‹¤.
    
    Args:
        entity: Weave entity (íŒ€ ë˜ëŠ” ì‚¬ìš©ì ì´ë¦„)
        project: Weave í”„ë¡œì íŠ¸ ì´ë¦„
        benchmarks: ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ ëª©ë¡ ì‚¬ìš©)
        name: ë¦¬ë”ë³´ë“œ ì´ë¦„
        description: ë¦¬ë”ë³´ë“œ ì„¤ëª…
    
    Returns:
        ë¦¬ë”ë³´ë“œ URL (ì„±ê³µ ì‹œ) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    print(f"\n{'='*60}")
    print(f"ğŸ† Weave Leaderboard ìƒì„±")
    print(f"{'='*60}")
    
    # ê¸°ë³¸ ë²¤ì¹˜ë§ˆí¬ ëª©ë¡
    DEFAULT_BENCHMARKS = [
        "ko_hellaswag",
        "ko_aime2025",
        "ifeval_ko",
        "ko_balt_700_syntax",
        "ko_balt_700_semantic",
        "haerae_bench_v1_rc",
        "haerae_bench_v1_wo_rc",
        "kmmlu",
        "kmmlu_pro",
        "squad_kor_v1",
        "ko_truthful_qa",
        "ko_moral",
        "ko_arc_agi",
        "ko_gsm8k",
        "korean_hate_speech",
        "kobbq",
        "ko_hle",
        "ko_hallulens_wikiqa",
        "ko_hallulens_longwiki",
        "ko_hallulens_nonexistent",
        "bfcl",
        "mtbench_ko",
        "swebench_verified_official_80",
    ]
    
    benchmarks = benchmarks or DEFAULT_BENCHMARKS
    
    # Weave ì´ˆê¸°í™”
    client = weave.get_client()
    if client is None:
        weave.init(f"{entity}/{project}")
        client = weave.get_client()
    
    if client is None:
        print("âŒ Weave í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return None
    
    try:
        # 1. LeaderboardColumn ìƒì„±
        print(f"ğŸ“Š {len(benchmarks)}ê°œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ LeaderboardColumn ìƒì„± ì¤‘...")
        new_columns = build_columns_from_benchmarks(benchmarks, entity, project)
        
        if not new_columns:
            print("âš ï¸ ìƒì„±í•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"   ìƒˆ ì»¬ëŸ¼: {len(new_columns)}ê°œ")
        
        # 3. ê¸°ì¡´ ë¦¬ë”ë³´ë“œ ê°€ì ¸ì˜¤ê¸° (ìˆë‹¤ë©´)
        existing_columns: list[leaderboard.LeaderboardColumn] = []
        try:
            existing = weave.ref(LEADERBOARD_REF).get()
            cols = getattr(existing, "columns", None)
            if cols:
                existing_columns = list(cols)
                print(f"   ê¸°ì¡´ ì»¬ëŸ¼: {len(existing_columns)}ê°œ")
        except Exception:
            print("   ê¸°ì¡´ ë¦¬ë”ë³´ë“œ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
        
        # 4. ì»¬ëŸ¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        merged_columns = list(
            {
                (
                    column.evaluation_object_ref,
                    column.scorer_name,
                    column.summary_metric_path,
                    column.should_minimize,
                ): column
                for column in (existing_columns or []) + new_columns
            }.values()
        )
        
        print(f"\nğŸ“ˆ ì´ {len(merged_columns)}ê°œ ì»¬ëŸ¼ìœ¼ë¡œ ë¦¬ë”ë³´ë“œ ìƒì„±")
        
        # 5. ë¦¬ë”ë³´ë“œ ìƒì„± ë° ë°œí–‰
        spec = leaderboard.Leaderboard(
            name=name,
            description=description,
            columns=merged_columns,
        )
        ref = weave.publish(spec, name=LEADERBOARD_REF)
        
        url = weave_urls.leaderboard_path(
            ref.entity,
            ref.project,
            ref.name,
        )
        
        print(f"\nâœ… Weave Leaderboard ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ”— URL: {url}")
        
        return url
        
    except Exception as e:
        print(f"âŒ Leaderboard ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


def create_weave_leaderboard_from_active_loggers(
    name: str = LEADERBOARD_NAME,
    description: str = LEADERBOARD_DESCRIPTION,
) -> str | None:
    """
    í™œì„±í™”ëœ EvaluationLoggerì—ì„œ Weave Leaderboard ìƒì„±
    
    ì´ í•¨ìˆ˜ëŠ” ê°™ì€ í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ evaluationì´ ì‹¤í–‰ëœ ê²½ìš°ì—ë§Œ ì‘ë™í•©ë‹ˆë‹¤.
    subprocessë¡œ ì‹¤í–‰ëœ ê²½ìš°ì—ëŠ” create_weave_leaderboard()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    
    Args:
        name: ë¦¬ë”ë³´ë“œ ì´ë¦„
        description: ë¦¬ë”ë³´ë“œ ì„¤ëª…
    
    Returns:
        ë¦¬ë”ë³´ë“œ URL (ì„±ê³µ ì‹œ) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    from weave.evaluation.eval_imperative import _active_evaluation_loggers
    from weave.trace.ref_util import get_ref
    
    client = weave.get_client()
    if client is None:
        print("âŒ Weave í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # í™œì„± loggerì—ì„œ ì»¬ëŸ¼ ë¹Œë“œ
        new_columns: list[leaderboard.LeaderboardColumn] = []
        
        for eval_logger in _active_evaluation_loggers:
            eval_output = eval_logger._evaluate_call and (eval_logger._evaluate_call.output or {})
            output_scorer = eval_output.get("output", {})
            
            for metric_name, metric_values in output_scorer.items():
                if not isinstance(metric_values, dict):
                    continue
                    
                for m_value in metric_values.keys():
                    if "err" in m_value.lower():
                        continue
                    
                    new_columns.append(
                        leaderboard.LeaderboardColumn(
                            evaluation_object_ref=get_ref(
                                eval_logger._pseudo_evaluation
                            ).uri(),
                            scorer_name="output",
                            summary_metric_path=f"{metric_name}.{m_value}",
                            should_minimize=False,
                        )
                    )
        
        if not new_columns:
            print("âš ï¸ í™œì„± evaluation loggerê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ê¸°ì¡´ ë¦¬ë”ë³´ë“œì™€ ë³‘í•©
        existing_columns: list[leaderboard.LeaderboardColumn] = []
        try:
            existing = weave.ref(LEADERBOARD_REF).get()
            cols = getattr(existing, "columns", None)
            if cols:
                existing_columns = list(cols)
        except Exception:
            pass
        
        merged_columns = list(
            {
                (
                    column.evaluation_object_ref,
                    column.scorer_name,
                    column.summary_metric_path,
                    column.should_minimize,
                ): column
                for column in (existing_columns or []) + new_columns
            }.values()
        )
        
        # ë¦¬ë”ë³´ë“œ ë°œí–‰
        spec = leaderboard.Leaderboard(
            name=name,
            description=description,
            columns=merged_columns,
        )
        ref = weave.publish(spec, name=LEADERBOARD_REF)
        
        url = weave_urls.leaderboard_path(
            ref.entity,
            ref.project,
            ref.name,
        )
        
        print(f"âœ… Weave Leaderboard ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ”— URL: {url}")
        
        return url
        
    except Exception as e:
        print(f"âŒ Leaderboard ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Weave Leaderboard ìƒì„±")
    parser.add_argument("--entity", "-e", required=True, help="Weave entity")
    parser.add_argument("--project", "-p", required=True, help="Weave project")
    parser.add_argument("--benchmarks", "-b", nargs="+", help="ë²¤ì¹˜ë§ˆí¬ ëª©ë¡ (ê¸°ë³¸: ì „ì²´)")
    parser.add_argument("--name", default=LEADERBOARD_NAME, help="ë¦¬ë”ë³´ë“œ ì´ë¦„")
    
    args = parser.parse_args()
    
    create_weave_leaderboard(
        entity=args.entity,
        project=args.project,
        benchmarks=args.benchmarks,
        name=args.name,
    )

