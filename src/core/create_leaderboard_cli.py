#!/usr/bin/env python3
"""
í‰ê°€ ê²°ê³¼ë¡œ ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„± CLI

ì´ ëª¨ë“ˆì€ Inspect AIë¡œ í‰ê°€í•œ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ì—¬
W&B ë¦¬ë”ë³´ë“œ í…Œì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ìˆ˜ë™ìœ¼ë¡œ ê²°ê³¼ ì…ë ¥í•˜ì—¬ ë¦¬ë”ë³´ë“œ ìƒì„±
    uv run python -m core.create_leaderboard_cli \
        --entity wandb-korea \
        --project korean-llm-eval \
        --model gpt-4o \
        --release-date 2024-05-13

    # Weave traceì—ì„œ ìë™ìœ¼ë¡œ ê²°ê³¼ ìˆ˜ì§‘
    uv run python -m core.create_leaderboard_cli \
        --entity wandb-korea \
        --project korean-llm-eval \
        --model gpt-4o \
        --from-weave

    # ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì…ë ¥
    uv run python -m core.create_leaderboard_cli \
        --entity wandb-korea \
        --project korean-llm-eval \
        --model gpt-4o \
        --results '{"ko_hle": {"score": 0.42}, "kmmlu": {"score": 0.78}}'
"""

import argparse
import json
import sys

from core.leaderboard_table import (
    LeaderboardTableBuilder,
    BENCHMARK_CONFIG,
)


def parse_args():
    parser = argparse.ArgumentParser(
        description="í‰ê°€ ê²°ê³¼ë¡œ ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„±",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
    # ìˆ˜ë™ìœ¼ë¡œ ê²°ê³¼ ì…ë ¥
    python -m core.create_leaderboard_cli \\
        --entity wandb-korea \\
        --project korean-llm-eval \\
        --model gpt-4o \\
        --results '{"ko_hle": {"score": 0.42}, "kmmlu": {"score": 0.78}}'

    # Weaveì—ì„œ ìë™ ìˆ˜ì§‘
    python -m core.create_leaderboard_cli \\
        --entity wandb-korea \\
        --project korean-llm-eval \\
        --model gpt-4o \\
        --from-weave

ì§€ì›ë˜ëŠ” ë²¤ì¹˜ë§ˆí¬:
    - ko_hle, ko_aime2025, ko_gsm8k (ì¶”ë¡ )
    - kmmlu, kmmlu_pro, haerae_bench_v1_rc/wo_rc (ì§€ì‹)
    - ifeval_ko, ko_balt_700, ko_hellaswag (ì–¸ì–´)
    - kobbq, ko_moral, korean_hate_speech (ì•ˆì „/í¸í–¥)
    - ko_hallulens_* (í™˜ê° ë°©ì§€)
    - bfcl, swebench_verified_official_80 (ë„êµ¬/ì½”ë”©)
    - mtbench_ko (ëŒ€í™”)
"""
    )
    
    parser.add_argument(
        "--entity", "-e",
        required=True,
        help="W&B entity (íŒ€ ë˜ëŠ” ì‚¬ìš©ì ì´ë¦„)"
    )
    parser.add_argument(
        "--project", "-p",
        required=True,
        help="W&B í”„ë¡œì íŠ¸ ì´ë¦„"
    )
    parser.add_argument(
        "--model", "-m",
        required=True,
        help="í‰ê°€ ëŒ€ìƒ ëª¨ë¸ ì´ë¦„"
    )
    parser.add_argument(
        "--release-date",
        default="unknown",
        help="ëª¨ë¸ ì¶œì‹œì¼ (YYYY-MM-DD í˜•ì‹)"
    )
    parser.add_argument(
        "--size-category",
        default="unknown",
        choices=["small", "medium", "large", "flagship", "unknown"],
        help="ëª¨ë¸ í¬ê¸° ì¹´í…Œê³ ë¦¬"
    )
    parser.add_argument(
        "--model-size",
        default="unknown",
        help="ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ (ì˜ˆ: 7B, 13B, 70B)"
    )
    parser.add_argument(
        "--results",
        type=str,
        help='ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ JSON ë¬¸ìì—´ (ì˜ˆ: \'{"ko_hle": {"score": 0.42}}\')'
    )
    parser.add_argument(
        "--results-file",
        type=str,
        help="ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ë‹´ê¸´ JSON íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--from-weave",
        action="store_true",
        help="Weave traceì—ì„œ ê²°ê³¼ ìë™ ìˆ˜ì§‘"
    )
    parser.add_argument(
        "--output", "-o",
        type=str,
        help="ê²°ê³¼ë¥¼ ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--no-wandb",
        action="store_true",
        help="W&Bì— ë¡œê¹…í•˜ì§€ ì•ŠìŒ (ë¡œì»¬ì—ì„œë§Œ í™•ì¸)"
    )
    parser.add_argument(
        "--list-benchmarks",
        action="store_true",
        help="ì§€ì›ë˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ëª©ë¡ ì¶œë ¥"
    )
    
    return parser.parse_args()


def list_benchmarks():
    """ì§€ì›ë˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ëª©ë¡ ì¶œë ¥"""
    print("\nğŸ“‹ ì§€ì›ë˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ëª©ë¡:")
    print("=" * 60)
    
    # GLP ê´€ë ¨ ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ¯ GLP (ë²”ìš©ì–¸ì–´ì„±ëŠ¥) ê´€ë ¨:")
    glp_benchmarks = []
    for name, config in BENCHMARK_CONFIG.items():
        mapper = config.get("mapper", {})
        for category in mapper.values():
            if category.startswith("GLP_"):
                glp_benchmarks.append((name, category))
                break
    
    for name, category in sorted(glp_benchmarks, key=lambda x: x[1]):
        print(f"  - {name:35} â†’ {category}")
    
    # ALT ê´€ë ¨ ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ›¡ï¸ ALT (ê°€ì¹˜ì •ë ¬ì„±ëŠ¥) ê´€ë ¨:")
    alt_benchmarks = []
    for name, config in BENCHMARK_CONFIG.items():
        mapper = config.get("mapper", {})
        for category in mapper.values():
            if category.startswith("ALT_"):
                alt_benchmarks.append((name, category))
                break
    
    for name, category in sorted(alt_benchmarks, key=lambda x: x[1]):
        print(f"  - {name:35} â†’ {category}")
    
    print("\n" + "=" * 60)


def create_leaderboard_cli():
    """CLI ì§„ì…ì """
    args = parse_args()
    
    if args.list_benchmarks:
        list_benchmarks()
        return
    
    print(f"\nğŸ† ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„±ê¸°")
    print(f"{'=' * 60}")
    print(f"Entity:  {args.entity}")
    print(f"Project: {args.project}")
    print(f"Model:   {args.model}")
    print(f"{'=' * 60}")
    
    # ë¹Œë” ìƒì„±
    builder = LeaderboardTableBuilder(
        entity=args.entity,
        project=args.project,
        model_name=args.model,
        release_date=args.release_date,
        size_category=args.size_category,
        model_size=args.model_size,
    )
    
    # ê²°ê³¼ ìˆ˜ì§‘
    if args.from_weave:
        print("\nğŸ” Weave traceì—ì„œ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...")
        builder.collect_from_weave_traces()
    
    if args.results:
        print("\nğŸ“¥ JSON ë¬¸ìì—´ì—ì„œ ê²°ê³¼ ë¡œë“œ ì¤‘...")
        try:
            results = json.loads(args.results)
            for benchmark_name, scores in results.items():
                builder.add_benchmark_result(benchmark_name, scores)
                print(f"  âœ“ {benchmark_name}: {scores}")
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            sys.exit(1)
    
    if args.results_file:
        print(f"\nğŸ“‚ íŒŒì¼ì—ì„œ ê²°ê³¼ ë¡œë“œ ì¤‘: {args.results_file}")
        try:
            with open(args.results_file, 'r') as f:
                results = json.load(f)
            for benchmark_name, scores in results.items():
                builder.add_benchmark_result(benchmark_name, scores)
                print(f"  âœ“ {benchmark_name}: {scores}")
        except (json.JSONDecodeError, FileNotFoundError) as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
            sys.exit(1)
    
    # ê²°ê³¼ í™•ì¸
    if not builder.benchmark_results:
        print("\nâŒ ìˆ˜ì§‘ëœ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   --results, --results-file, ë˜ëŠ” --from-weave ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    print(f"\nğŸ“Š ìˆ˜ì§‘ëœ ë²¤ì¹˜ë§ˆí¬: {len(builder.benchmark_results)}ê°œ")
    
    # ë¦¬ë”ë³´ë“œ ìƒì„±
    if args.no_wandb:
        print("\nğŸ“‹ ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„± ì¤‘ (W&B ë¡œê¹… ì—†ìŒ)...")
        df = builder.build_leaderboard_df()
    else:
        print("\nğŸ“‹ ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„± ë° W&B ë¡œê¹… ì¤‘...")
        df = builder.build_and_log()
        builder.finish()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ë¦¬ë”ë³´ë“œ í…Œì´ë¸”:")
    print("=" * 60)
    
    # ì£¼ìš” ì ìˆ˜ ì¶œë ¥
    if 'FINAL_SCORE' in df.columns:
        print(f"\nğŸ† FINAL_SCORE: {df['FINAL_SCORE'].iloc[0]:.4f}")
    if 'ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG' in df.columns:
        print(f"   GLP í‰ê· : {df['ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG'].iloc[0]:.4f}")
    if 'ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG' in df.columns:
        print(f"   ALT í‰ê· : {df['ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG'].iloc[0]:.4f}")
    
    print("\nğŸ“‹ ì „ì²´ í…Œì´ë¸”:")
    print(df.T.to_string())
    
    # CSV ì €ì¥
    if args.output:
        df.to_csv(args.output, index=False)
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("\nâœ… ì™„ë£Œ!")
    
    return df


if __name__ == "__main__":
    create_leaderboard_cli()

