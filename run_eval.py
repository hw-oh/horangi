#!/usr/bin/env python3
"""
ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë° ë¦¬ë”ë³´ë“œ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ ì‹¤í–‰ (5ìƒ˜í”Œì”©) - configs/models/gpt-4o.yaml ì„¤ì • ìë™ ì ìš©
    uv run python run_eval.py --model openai/gpt-4o-mini

    # ë” ë§ì€ ìƒ˜í”Œë¡œ ì‹¤í–‰
    uv run python run_eval.py --model openai/gpt-4o --limit 10

    # íŠ¹ì • ë²¤ì¹˜ë§ˆí¬ ì œì™¸
    uv run python run_eval.py --model openai/gpt-4o --exclude haerae_bench_v1_rc,bfcl_text

    # íŠ¹ì • ë²¤ì¹˜ë§ˆí¬ë§Œ ì‹¤í–‰
    uv run python run_eval.py --model openai/gpt-4o --only ko_hle,kmmlu,kobbq

    # W&B ë¦¬ë”ë³´ë“œ ìë™ ìƒì„±
    uv run python run_eval.py --model openai/gpt-4o \
        --entity wandb-korea \
        --project korean-llm-eval \
        --create-leaderboard

    # ë¦¬ë”ë³´ë“œ ìƒì„± + ëª¨ë¸ ë©”íƒ€ë°ì´í„° (ì„¤ì • íŒŒì¼ì—ì„œ ìë™ ë¡œë“œ)
    uv run python run_eval.py --model openai/gpt-4o \
        --entity wandb-korea \
        --project korean-llm-eval \
        --create-leaderboard

Note:
    ëª¨ë¸ ì„¤ì •ì€ configs/models/<model_name>.yaml íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
    - base_url: API ì—”ë“œí¬ì¸íŠ¸ (OPENAI_BASE_URL ë“±ìœ¼ë¡œ ì„¤ì •ë¨)
    - api_key_env: API í‚¤ í™˜ê²½ë³€ìˆ˜ ì´ë¦„
    - metadata: release_date, size_category ë“± ë¦¬ë”ë³´ë“œìš© ë©”íƒ€ë°ì´í„°
"""

import argparse
import json
import os
import re
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# src í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / "src"))

import wandb
from core.config_loader import get_config

# ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ëª©ë¡ (í™œì„±í™”ëœ ê²ƒë§Œ)
ALL_BENCHMARKS = [
    "ko_hellaswag",
    "ko_aime2025",
    "ifeval_ko",
    "ko_balt_700_syntax",
    "ko_balt_700_semantic",
    "haerae_bench_v1_rc",
    "haerae_bench_v1_wo_rc",
    "kmmlu",
    "kmmlu_pro",
    "squad_kor_v1",
    "ko_truthful_qa",
    "ko_moral",
    "ko_arc_agi",
    "ko_gsm8k",
    "korean_hate_speech",
    "kobbq",
    "ko_hle",
    "ko_hallulens_wikiqa",
    "ko_hallulens_longwiki",
    "ko_hallulens_nonexistent",
    "bfcl",
    "mtbench_ko",
    "swebench_verified_official_80",
]

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ë²¤ì¹˜ë§ˆí¬ (ê°€ë²¼ìš´ ê²ƒë“¤ë§Œ)
QUICK_BENCHMARKS = [
    "ko_hellaswag",
    "kmmlu",
    "kobbq",
    "korean_hate_speech",
    "ifeval_ko",
    "ko_moral",
]


def get_model_env(model: str) -> dict[str, str]:
    """
    ëª¨ë¸ ì„¤ì • íŒŒì¼ì—ì„œ API í™˜ê²½ë³€ìˆ˜ ìƒì„±
    
    configs/models/<model_name>.yaml íŒŒì¼ì—ì„œ:
    - base_url â†’ OPENAI_BASE_URL (ë˜ëŠ” providerë³„ í™˜ê²½ë³€ìˆ˜)
    - api_key_env â†’ í•´ë‹¹ í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
    
    Returns:
        í™˜ê²½ë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬
    """
    config = get_config()
    model_config = config.get_model(model)
    
    if not model_config:
        return {}
    
    env = {}
    
    # Provider í™•ì¸ (openai/gpt-4o â†’ openai)
    provider = model.split("/")[0] if "/" in model else "openai"
    provider_upper = provider.upper()
    
    # Base URL ì„¤ì •
    base_url = model_config.get("base_url") or model_config.get("api_base")
    if base_url:
        # OpenAI í˜¸í™˜ APIëŠ” OPENAI_BASE_URL ì‚¬ìš©
        if provider in ["openai", "together", "groq", "fireworks"]:
            env["OPENAI_BASE_URL"] = base_url
        else:
            env[f"{provider_upper}_BASE_URL"] = base_url
    
    # API í‚¤ ì„¤ì •
    api_key_env = model_config.get("api_key_env")
    if api_key_env:
        api_key = os.environ.get(api_key_env)
        if api_key:
            # OpenAI í˜¸í™˜ API
            if provider in ["openai", "together", "groq", "fireworks"]:
                env["OPENAI_API_KEY"] = api_key
            else:
                env[f"{provider_upper}_API_KEY"] = api_key
    
    return env


def get_model_metadata(model: str) -> dict:
    """
    ëª¨ë¸ ì„¤ì • íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    
    Returns:
        {
            "release_date": "2024-05-13",
            "size_category": "flagship",
            "model_size": "unknown",
            ...
        }
    """
    config = get_config()
    model_config = config.get_model(model)
    
    if not model_config:
        return {}
    
    metadata = model_config.get("metadata", {})
    return {
        "release_date": metadata.get("release_date", "unknown"),
        "size_category": metadata.get("size_category", "unknown"),
        "model_size": metadata.get("model_size") or metadata.get("parameters", "unknown"),
    }


def run_benchmark(benchmark: str, model: str, limit: int) -> tuple[str, bool, str, dict | None]:
    """
    ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    
    ëª¨ë¸ ì„¤ì • íŒŒì¼(configs/models/<model>.yaml)ì˜ API ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤.
    
    Returns:
        (benchmark_name, success, error_message, scores)
    """
    cmd = [
        "uv", "run", "horangi",
        benchmark,
        "--model", model,
        "-T", f"limit={limit}",
    ]
    
    # ëª¨ë¸ ì„¤ì •ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    model_env = get_model_env(model)
    
    # í˜„ì¬ í™˜ê²½ë³€ìˆ˜ì™€ ë³‘í•© (ëª¨ë¸ ì„¤ì •ì´ ìš°ì„ )
    env = os.environ.copy()
    env.update(model_env)
    
    print(f"\n{'='*60}")
    print(f"ğŸƒ Running: {benchmark}")
    print(f"   Command: {' '.join(cmd)}")
    print(f"{'='*60}")
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=1800,  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
            env=env,  # ëª¨ë¸ ì„¤ì •ì´ ì ìš©ëœ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
        )
        
        success = result.returncode == 0
        
        # stdout ì¶œë ¥
        if result.stdout:
            print(result.stdout)
        if result.stderr:
            print(result.stderr, file=sys.stderr)
        
        # ì ìˆ˜ íŒŒì‹± ì‹œë„
        scores = None
        if success:
            scores = parse_scores_from_output(result.stdout + result.stderr, benchmark)
        
        return benchmark, success, "" if success else f"Exit code: {result.returncode}", scores
    
    except subprocess.TimeoutExpired:
        return benchmark, False, "Timeout (30m)", None
    except Exception as e:
        return benchmark, False, str(e), None


def parse_scores_from_output(output: str, benchmark: str) -> dict | None:
    """
    Inspect AI ì¶œë ¥ì—ì„œ ì ìˆ˜ íŒŒì‹±
    
    Inspect AI ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
        accuracy  0.600
        stderr    0.245
        
        ë˜ëŠ”
        
        mean    0.640
        writing_score  0.640
    """
    scores = {}
    
    # Inspect AI ì¶œë ¥ í˜•ì‹: "metric_name  value" (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„, ì¤„ ì‹œì‘)
    # ê° ë©”íŠ¸ë¦­ë³„ íŒ¨í„´ (ì´ë¦„, ì •ê·œì‹)
    metric_patterns = [
        ("accuracy", r"^accuracy\s+([\d.]+)", False),
        ("mean", r"^mean\s+([\d.]+)", False),  # mtbench
        ("macro_f1", r"macro_f1\s+([\d.]+)", False),
        ("f1", r"^f1\s+([\d.]+)", False),
        ("resolved", r"resolved\s+([\d.]+)", False),  # swebench
        ("refusal_rate", r"refusal_rate\s+([\d.]+)", False),  # hallulens
        ("correct_rate", r"correct_rate\s+([\d.]+)", False),  # hallulens
        ("kobbq_avg", r"kobbq_avg\s+([\d.]+)", False),  # kobbq
        ("final_acc", r"final_acc\s+([\d.]+)", False),  # ifeval
        ("prompt_strict_acc", r"prompt_strict_acc\s+([\d.]+)", False),  # ifeval
        ("hle_accuracy", r"hle_accuracy\s+([\d.]+)", False),  # hle
    ]
    
    for metric_name, pattern, _ in metric_patterns:
        match = re.search(pattern, output, re.MULTILINE | re.IGNORECASE)
        if match:
            try:
                scores[metric_name] = float(match.group(1))
            except ValueError:
                pass
    
    # ë²¤ì¹˜ë§ˆí¬ë³„ ì£¼ìš” ì ìˆ˜ ì„ íƒ
    # IFEval: final_acc ë˜ëŠ” prompt_strict_acc ì‚¬ìš©
    if benchmark == "ifeval_ko":
        if "final_acc" in scores:
            return {"score": scores["final_acc"]}
        elif "prompt_strict_acc" in scores:
            return {"score": scores["prompt_strict_acc"]}
    
    # KoBBQ: kobbq_avg ì‚¬ìš©
    if benchmark == "kobbq" and "kobbq_avg" in scores:
        return {"score": scores["kobbq_avg"]}
    
    # HLE: hle_accuracy ì‚¬ìš©
    if benchmark == "ko_hle" and "hle_accuracy" in scores:
        return {"score": scores["hle_accuracy"]}
    
    # HalluLens: correct_rate ë˜ëŠ” refusal_rate ì‚¬ìš©
    if "hallulens" in benchmark:
        if "correct_rate" in scores:
            return {"score": scores["correct_rate"]}
        elif "refusal_rate" in scores:
            return {"score": scores["refusal_rate"]}
    
    # MT-Bench: mean ì‚¬ìš© (10ì  ë§Œì  â†’ 0-1 ìŠ¤ì¼€ì¼)
    if benchmark == "mtbench_ko" and "mean" in scores:
        return {"score": scores["mean"] / 10.0}
    
    # ì¼ë°˜ì ì¸ ë©”íŠ¸ë¦­ ìš°ì„ ìˆœìœ„
    if "accuracy" in scores:
        return {"score": scores["accuracy"]}
    elif "mean" in scores:
        return {"score": scores["mean"] / 10.0}
    elif "macro_f1" in scores:
        return {"score": scores["macro_f1"]}
    elif "f1" in scores:
        return {"score": scores["f1"]}
    elif "resolved" in scores:
        return {"score": scores["resolved"]}
    
    return None


def create_leaderboard(
    model: str,
    benchmark_scores: dict[str, dict],
    entity: str,
    project: str,
    release_date: str = "unknown",
    size_category: str = "unknown",
    model_size: str = "unknown",
    wandb_run=None,
    output_csv: str | None = None,
):
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¡œ ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„±
    
    Args:
        wandb_run: ê¸°ì¡´ W&B run ê°ì²´ (ìˆìœ¼ë©´ í•´ë‹¹ runì— ë¡œê¹…, ì—†ìœ¼ë©´ ìƒˆ run ìƒì„±)
    """
    from core.leaderboard_table import LeaderboardTableBuilder
    
    print(f"\n{'='*60}")
    print(f"ğŸ† ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„±")
    print(f"{'='*60}")
    
    # ëª¨ë¸ ì´ë¦„ì—ì„œ provider ì œê±° (openai/gpt-4o â†’ gpt-4o)
    model_name = model.split("/")[-1] if "/" in model else model
    
    builder = LeaderboardTableBuilder(
        entity=entity,
        project=project,
        model_name=model_name,
        release_date=release_date,
        size_category=size_category,
        model_size=model_size,
    )
    
    # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶”ê°€
    for benchmark_name, scores in benchmark_scores.items():
        if scores:
            builder.add_benchmark_result(benchmark_name, scores)
            print(f"  âœ“ {benchmark_name}: {scores}")
    
    if not builder.benchmark_results:
        print("âŒ ìˆ˜ì§‘ëœ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"\nğŸ“Š ìˆ˜ì§‘ëœ ë²¤ì¹˜ë§ˆí¬: {len(builder.benchmark_results)}ê°œ")
    
    # ë¦¬ë”ë³´ë“œ DataFrame ìƒì„±
    print("\nğŸ“‹ ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„± ì¤‘...")
    try:
        df = builder.build_leaderboard_df()
        glp_radar, glp_detail, alt_radar, alt_detail = builder.build_radar_tables(df)
    except Exception as e:
        print(f"âš ï¸ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    # W&Bì— ë¡œê¹… (ê¸°ì¡´ run ì‚¬ìš©)
    if wandb_run is not None:
        print("ğŸ“¤ W&B runì— ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ë¡œê¹… ì¤‘...")
        try:
            # ë¦¬ë”ë³´ë“œ í…Œì´ë¸”
            leaderboard_table = wandb.Table(dataframe=df)
            wandb_run.log({"leaderboard_table": leaderboard_table})
            
            # ë ˆì´ë” í…Œì´ë¸”
            wandb_run.log({
                "glp_radar_table": wandb.Table(dataframe=glp_radar),
                "glp_detail_radar_table": wandb.Table(dataframe=glp_detail),
                "alt_radar_table": wandb.Table(dataframe=alt_radar),
                "alt_detail_radar_table": wandb.Table(dataframe=alt_detail),
            })
            
            # Summaryì— ì£¼ìš” ì ìˆ˜ ì €ì¥
            if 'FINAL_SCORE' in df.columns and len(df) > 0:
                score = df['FINAL_SCORE'].iloc[0]
                if score == score:  # not NaN
                    wandb_run.summary["FINAL_SCORE"] = score
            if 'ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG' in df.columns and len(df) > 0:
                score = df['ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG'].iloc[0]
                if score == score:
                    wandb_run.summary["GLP_AVG"] = score
            if 'ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG' in df.columns and len(df) > 0:
                score = df['ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG'].iloc[0]
                if score == score:
                    wandb_run.summary["ALT_AVG"] = score
            
            print("âœ… W&B ë¡œê¹… ì™„ë£Œ!")
        except Exception as e:
            print(f"âš ï¸ W&B ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ë¦¬ë”ë³´ë“œ í…Œì´ë¸”:")
    print("=" * 60)
    
    # ì£¼ìš” ì ìˆ˜ ì¶œë ¥
    if 'FINAL_SCORE' in df.columns and len(df) > 0:
        score = df['FINAL_SCORE'].iloc[0]
        if not (score != score):  # NaN check
            print(f"\nğŸ† FINAL_SCORE: {score:.4f}")
    if 'ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG' in df.columns and len(df) > 0:
        score = df['ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG'].iloc[0]
        if not (score != score):
            print(f"   GLP í‰ê· : {score:.4f}")
    if 'ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG' in df.columns and len(df) > 0:
        score = df['ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG'].iloc[0]
        if not (score != score):
            print(f"   ALT í‰ê· : {score:.4f}")
    
    print("\nğŸ“‹ ì „ì²´ í…Œì´ë¸”:")
    print(df.T.to_string())
    
    # CSV ì €ì¥
    if output_csv:
        df.to_csv(output_csv, index=False)
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ {output_csv}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return df


def main():
    parser = argparse.ArgumentParser(
        description="Run benchmarks and create leaderboard",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
    # ê¸°ë³¸ ì‹¤í–‰
    uv run python run_eval.py --model openai/gpt-4o-mini

    # ì „ì²´ ë²¤ì¹˜ë§ˆí¬ + ë¦¬ë”ë³´ë“œ ìƒì„±
    uv run python run_eval.py --model openai/gpt-4o \\
        --entity wandb-korea \\
        --project korean-llm-eval \\
        --create-leaderboard \\
        --release-date 2024-05-13

    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê°€ë²¼ìš´ ë²¤ì¹˜ë§ˆí¬ë§Œ)
    uv run python run_eval.py --model openai/gpt-4o-mini --quick
"""
    )
    
    # ê¸°ë³¸ ì˜µì…˜
    parser.add_argument("--model", type=str, required=True, 
                        help="Model to use (e.g., openai/gpt-4o-mini)")
    parser.add_argument("--limit", type=int, default=5, 
                        help="Number of samples per benchmark (default: 5)")
    parser.add_argument("--exclude", type=str, default="", 
                        help="Comma-separated list of benchmarks to exclude")
    parser.add_argument("--only", type=str, default="", 
                        help="Comma-separated list of benchmarks to run (exclusive)")
    parser.add_argument("--quick", action="store_true",
                        help="Run only quick/light benchmarks")
    parser.add_argument("--dry-run", action="store_true", 
                        help="Print commands without running")
    
    # ë¦¬ë”ë³´ë“œ ì˜µì…˜
    parser.add_argument("--create-leaderboard", action="store_true",
                        help="Create leaderboard table after running benchmarks")
    parser.add_argument("--entity", "-e", type=str, default="",
                        help="W&B entity (required for leaderboard)")
    parser.add_argument("--project", "-p", type=str, default="",
                        help="W&B project (required for leaderboard)")
    parser.add_argument("--release-date", type=str, default="unknown",
                        help="Model release date (YYYY-MM-DD)")
    parser.add_argument("--size-category", type=str, default="unknown",
                        choices=["small", "medium", "large", "flagship", "unknown"],
                        help="Model size category")
    parser.add_argument("--model-size", type=str, default="unknown",
                        help="Model parameter count (e.g., 7B, 13B, 70B)")
    parser.add_argument("--no-wandb", action="store_true",
                        help="Don't log to W&B (local only)")
    parser.add_argument("--output-csv", type=str,
                        help="Save leaderboard to CSV file")
    
    args = parser.parse_args()
    
    # ë¦¬ë”ë³´ë“œ ìƒì„± ì‹œ entity/project í•„ìˆ˜
    if args.create_leaderboard and not args.no_wandb:
        if not args.entity or not args.project:
            print("âŒ ë¦¬ë”ë³´ë“œ ìƒì„± ì‹œ --entityì™€ --projectê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("   ë˜ëŠ” --no-wandb ì˜µì…˜ìœ¼ë¡œ W&B ë¡œê¹… ì—†ì´ ë¡œì»¬ì—ì„œë§Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            sys.exit(1)
    
    # ë²¤ì¹˜ë§ˆí¬ í•„í„°ë§
    if args.quick:
        benchmarks = QUICK_BENCHMARKS
    elif args.only:
        benchmarks = [b.strip() for b in args.only.split(",") if b.strip()]
        # ìœ íš¨ì„± ê²€ì‚¬
        invalid = [b for b in benchmarks if b not in ALL_BENCHMARKS]
        if invalid:
            print(f"âŒ Unknown benchmarks: {invalid}")
            print(f"   Available: {ALL_BENCHMARKS}")
            sys.exit(1)
    else:
        exclude_list = [b.strip() for b in args.exclude.split(",") if b.strip()]
        benchmarks = [b for b in ALL_BENCHMARKS if b not in exclude_list]
    
    # ëª¨ë¸ ì´ë¦„ì—ì„œ provider ì œê±° (openai/gpt-4o â†’ gpt-4o)
    model_name = args.model.split("/")[-1] if "/" in args.model else args.model
    
    print(f"\nğŸ¯ Horangi Benchmark Runner")
    print(f"{'='*60}")
    print(f"Model: {args.model}")
    print(f"Limit: {args.limit} samples per benchmark")
    print(f"Benchmarks: {len(benchmarks)} / {len(ALL_BENCHMARKS)}")
    if args.create_leaderboard:
        print(f"Leaderboard: {args.entity}/{args.project}")
    print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    if args.dry_run:
        print("\nğŸ” Dry run - commands that would be executed:")
        for benchmark in benchmarks:
            cmd = f"uv run horangi {benchmark} --model {args.model} -T limit={args.limit}"
            print(f"  {cmd}")
        return
    
    # W&B run ì´ˆê¸°í™” (ë¦¬ë”ë³´ë“œ ìƒì„± ì‹œ)
    wandb_run = None
    if args.create_leaderboard and not args.no_wandb:
        print(f"\nğŸ“Š W&B run ì´ˆê¸°í™” ì¤‘...")
        try:
            wandb_run = wandb.init(
                entity=args.entity,
                project=args.project,
                name=f"eval-{model_name}-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                job_type="evaluation",
                config={
                    "model": args.model,
                    "model_name": model_name,
                    "limit": args.limit,
                    "benchmarks": benchmarks,
                },
            )
            print(f"âœ… W&B run ì‹œì‘: {wandb_run.url}")
        except Exception as e:
            print(f"âš ï¸ W&B ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("   ë¡œì»¬ì—ì„œë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            wandb_run = None
    
    # ì‹¤í–‰ ê²°ê³¼ ì¶”ì 
    results = []
    benchmark_scores = {}
    
    for i, benchmark in enumerate(benchmarks, 1):
        print(f"\n[{i}/{len(benchmarks)}] ", end="")
        name, success, error, scores = run_benchmark(benchmark, args.model, args.limit)
        results.append((name, success, error))
        
        if scores:
            benchmark_scores[name] = scores
            
            # W&Bì— ê°œë³„ ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ ë¡œê¹…
            if wandb_run is not None:
                try:
                    wandb_run.log({
                        f"benchmark/{name}": scores.get("score", 0),
                    })
                except Exception:
                    pass
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n\n{'='*60}")
    print(f"ğŸ“Š Results Summary")
    print(f"{'='*60}")
    
    successful = [r for r in results if r[1]]
    failed = [r for r in results if not r[1]]
    
    print(f"\nâœ… Successful: {len(successful)} / {len(results)}")
    for name, _, _ in successful:
        score_info = benchmark_scores.get(name, {})
        score_str = f" (score: {score_info.get('score', 'N/A')})" if score_info else ""
        print(f"   - {name}{score_str}")
    
    if failed:
        print(f"\nâŒ Failed: {len(failed)} / {len(results)}")
        for name, _, error in failed:
            print(f"   - {name}: {error}")
    
    # W&B summaryì— ì„±ê³µ/ì‹¤íŒ¨ ìˆ˜ ê¸°ë¡
    if wandb_run is not None:
        wandb_run.summary["successful_benchmarks"] = len(successful)
        wandb_run.summary["failed_benchmarks"] = len(failed)
        wandb_run.summary["total_benchmarks"] = len(results)
    
    # ë¦¬ë”ë³´ë“œ ìƒì„± (ì„±ê³µí•œ ë²¤ì¹˜ë§ˆí¬ê°€ ìˆìœ¼ë©´)
    if args.create_leaderboard and benchmark_scores:
        # ëª¨ë¸ ì„¤ì •ì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ (CLI ì¸ìê°€ ì—†ìœ¼ë©´ ì„¤ì • íŒŒì¼ ì‚¬ìš©)
        model_metadata = get_model_metadata(args.model)
        
        release_date = args.release_date if args.release_date != "unknown" else model_metadata.get("release_date", "unknown")
        size_category = args.size_category if args.size_category != "unknown" else model_metadata.get("size_category", "unknown")
        model_size = args.model_size if args.model_size != "unknown" else model_metadata.get("model_size", "unknown")
        
        # 1. W&B Models í…Œì´ë¸” ë¦¬ë”ë³´ë“œ ìƒì„±
        create_leaderboard(
            model=args.model,
            benchmark_scores=benchmark_scores,
            entity=args.entity or "local",
            project=args.project or "benchmark-results",
            release_date=release_date,
            size_category=size_category,
            model_size=model_size,
            wandb_run=wandb_run,
            output_csv=args.output_csv,
        )
        
        # 2. Weave Leaderboard ìƒì„± (ë³„ë„ ê¸°ëŠ¥)
        if args.entity and args.project:
            try:
                from core.weave_leaderboard import create_weave_leaderboard
                # ì„±ê³µí•œ ë²¤ì¹˜ë§ˆí¬ ëª©ë¡ë§Œ ì „ë‹¬
                successful_benchmarks = list(benchmark_scores.keys())
                create_weave_leaderboard(
                    entity=args.entity,
                    project=args.project,
                    benchmarks=successful_benchmarks,
                )
            except Exception as e:
                print(f"âš ï¸ Weave Leaderboard ìƒì„± ì‹¤íŒ¨: {e}")
    
    # W&B run ì¢…ë£Œ
    if wandb_run is not None:
        print(f"\nğŸ“Š W&B run ì¢…ë£Œ ì¤‘...")
        wandb_run.finish()
        print(f"âœ… W&B run ì™„ë£Œ!")
    
    print(f"\n{'='*60}")
    print(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    # ì‹¤íŒ¨ê°€ ìˆì–´ë„ ë¦¬ë”ë³´ë“œê°€ ìƒì„±ë˜ì—ˆìœ¼ë©´ exit code 0
    # (ì¼ë¶€ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨í•´ë„ ê²°ê³¼ëŠ” ì €ì¥ë¨)
    sys.exit(0)


if __name__ == "__main__":
    main()
