#!/usr/bin/env python3

import argparse
import locale
import os
import re
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# inspect_evalsì˜ ë‚ ì§œ íŒŒì‹± ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì˜ì–´ ë¡œì¼€ì¼ ì„¤ì •
try:
    locale.setlocale(locale.LC_TIME, "en_US.UTF-8")
except locale.Error:
    try:
        locale.setlocale(locale.LC_TIME, "C")
    except locale.Error:
        pass  # ë¡œì¼€ì¼ ì„¤ì • ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

# src í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / "src"))

import wandb
from core.config_loader import get_config

# ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ëª©ë¡ (í™œì„±í™”ëœ ê²ƒë§Œ)
ALL_BENCHMARKS = [
    "ko_hellaswag",
    "ko_aime2025",
    "ifeval_ko",
    "ko_balt_700_syntax",
    "ko_balt_700_semantic",
    "haerae_bench_v1_rc",
    "haerae_bench_v1_wo_rc",
    "kmmlu",
    "kmmlu_pro",
    "squad_kor_v1",
    "ko_truthful_qa",
    "ko_moral",
    "ko_arc_agi",
    "ko_gsm8k",
    "korean_hate_speech",
    "kobbq",
    "ko_hle",
    "ko_hallulens_wikiqa",
    "ko_hallulens_longwiki",
    "ko_hallulens_nonexistent",
    "bfcl",
    "mtbench_ko",
    "swebench_verified_official_80",
]

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ë²¤ì¹˜ë§ˆí¬ (ê°€ë²¼ìš´ ê²ƒë“¤ë§Œ)
QUICK_BENCHMARKS = [
    "ko_hellaswag",
    "kmmlu",
    "kobbq",
    "korean_hate_speech",
    "ifeval_ko",
    "ko_moral",
]


def get_model_env(config_name: str) -> dict[str, str]:
    """
    ëª¨ë¸ ì„¤ì • íŒŒì¼ì—ì„œ API í™˜ê²½ë³€ìˆ˜ ìƒì„±
    
    configs/models/<config_name>.yaml íŒŒì¼ì—ì„œ:
    - base_url â†’ OPENAI_BASE_URL (ë˜ëŠ” providerë³„ í™˜ê²½ë³€ìˆ˜)
    - api_key_env â†’ í•´ë‹¹ í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
    
    Returns:
        í™˜ê²½ë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬
    """
    config = get_config()
    model_config = config.get_model(config_name)
    
    if not model_config:
        return {}
    
    env = {}
    
    # Provider í™•ì¸ (model_id ê¸°ì¤€: openai/solar-pro2 â†’ openai)
    model_id = model_config.get("model_id") or config_name
    provider = model_id.split("/")[0] if "/" in model_id else "openai"
    provider_upper = provider.upper()
    
    # Base URL ì„¤ì •
    base_url = model_config.get("base_url") or model_config.get("api_base")
    if base_url:
        # OpenAI í˜¸í™˜ APIëŠ” OPENAI_BASE_URL ì‚¬ìš©
        if provider in ["openai", "together", "groq", "fireworks"]:
            env["OPENAI_BASE_URL"] = base_url
        else:
            env[f"{provider_upper}_BASE_URL"] = base_url
    
    # API í‚¤ ì„¤ì •
    api_key_env = model_config.get("api_key_env")
    if api_key_env:
        api_key = os.environ.get(api_key_env)
        if api_key:
            # OpenAI í˜¸í™˜ API
            if provider in ["openai", "together", "groq", "fireworks"]:
                env["OPENAI_API_KEY"] = api_key
            else:
                env[f"{provider_upper}_API_KEY"] = api_key
    
    return env


def get_model_metadata(model: str) -> dict:
    """
    ëª¨ë¸ ì„¤ì • íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    
    Returns:
        {
            "release_date": "2024-05-13",
            "size_category": "flagship",
            "model_size": "unknown",
            ...
        }
    """
    config = get_config()
    model_config = config.get_model(model)
    
    if not model_config:
        return {}
    
    metadata = model_config.get("metadata", {})
    return {
        "release_date": metadata.get("release_date", "unknown"),
        "size_category": metadata.get("size_category", "unknown"),
        "model_size": metadata.get("model_size") or metadata.get("parameters", "unknown"),
    }


def run_benchmark(
    benchmark: str, 
    config_name: str,
    limit: int | None,
    wandb_entity: str | None = None,
    wandb_project: str | None = None,
) -> tuple[str, bool, str, dict | None]:
    """
    ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    
    ëª¨ë¸ ì„¤ì • íŒŒì¼(configs/models/<model>.yaml)ì˜ API ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤.
    
    Returns:
        (benchmark_name, success, error_message, scores)
    """
    cmd = ["uv", "run", "horangi", benchmark, "--config", config_name]
    
    # limitì´ ì§€ì •ëœ ê²½ìš°ì—ë§Œ ì¶”ê°€ (null = ì „ì²´)
    if limit is not None:
        cmd.extend(["-T", f"limit={limit}"])
    
    # ëª¨ë¸ ì„¤ì •ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    model_env = get_model_env(config_name)
    
    # í˜„ì¬ í™˜ê²½ë³€ìˆ˜ì™€ ë³‘í•© (ëª¨ë¸ ì„¤ì •ì´ ìš°ì„ )
    env = os.environ.copy()
    env.update(model_env)
    
    # inspect_evalsì˜ ë‚ ì§œ íŒŒì‹± ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì˜ì–´ ë¡œì¼€ì¼ ì„¤ì •
    env["LC_TIME"] = "en_US.UTF-8"

    # ê° ë²¤ì¹˜ë§ˆí¬ subprocess(inspect eval)ê°€ ê¸°ë¡í•  W&B/Weave í”„ë¡œì íŠ¸ ê°•ì œ ì§€ì •
    # (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ wandbì˜ ê¸°ë³¸ project(ì˜ˆ: horangi-dev)ë¡œ ê¸°ë¡ë  ìˆ˜ ìˆìŒ)
    if wandb_entity:
        env["WANDB_ENTITY"] = wandb_entity
    if wandb_project:
        env["WANDB_PROJECT"] = wandb_project
    
    print(f"\n{'='*60}")
    print(f"ğŸƒ Running: {benchmark}")
    print(f"   Command: {' '.join(cmd)}")
    print(f"{'='*60}")
    
    try:
        # ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•´ Popen ì‚¬ìš©
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,  # stderrë¥¼ stdoutì— ë³‘í•©
            text=True,
            bufsize=1,  # ë¼ì¸ ë²„í¼ë§
            env=env,
        )
        
        # ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•˜ë©´ì„œ ê²°ê³¼ ìˆ˜ì§‘
        output_lines = []
        weave_eval_url: str | None = None
        hook_noise_patterns = (
            r"^inspect_ai v",
            r"^- hooks enabled:",
            r"^\s*inspect_wandb/weave_evaluation_hooks:",
            r"^\s*inspect_wandb/wandb_models_hooks:",
        )
        for line in process.stdout:
            # Weave Eval URLì€ ë²¤ì¹˜ë§ˆí¬ ì¢…ë£Œ í›„ì— í•œ ë²ˆë§Œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ìº¡ì²˜ë§Œ í•¨
            m = re.search(r"ğŸ”—\s*Weave Eval:\s*(https?://\S+)", line)
            if m:
                weave_eval_url = m.group(1)
            
            # ë¶ˆí•„ìš”í•œ ì¡ìŒ ë¡œê·¸/ì¤‘ê°„ URL ë¼ì¸ í•„í„°ë§
            suppress = False
            if m:
                suppress = True
            else:
                for pat in hook_noise_patterns:
                    if re.search(pat, line):
                        suppress = True
                        break
            
            if not suppress:
                print(line, end="", flush=True)  # ì‹¤ì‹œê°„ ì¶œë ¥
            output_lines.append(line)
        
        process.wait(timeout=1800)  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        full_output = "".join(output_lines)
        
        success = process.returncode == 0
        
        # ë²¤ì¹˜ë§ˆí¬ ì¢…ë£Œ í›„ Weave Eval URL ì¶œë ¥
        if weave_eval_url:
            print(f"\nğŸ”— Weave Eval: {weave_eval_url}")
        
        # ì ìˆ˜ íŒŒì‹± ì‹œë„
        scores = None
        if success:
            scores = parse_scores_from_output(full_output, benchmark)
        
        return benchmark, success, "" if success else f"Exit code: {process.returncode}", scores
    
    except subprocess.TimeoutExpired:
        process.kill()
        return benchmark, False, "Timeout (30m)", None
    except Exception as e:
        if 'process' in locals():
            process.kill()
        return benchmark, False, str(e), None


def parse_scores_from_output(output: str, benchmark: str) -> dict | None:
    """
    Inspect AI ì¶œë ¥ì—ì„œ ì ìˆ˜ íŒŒì‹±
    
    Inspect AI ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
        accuracy  0.600
        stderr    0.245
        
        ë˜ëŠ”
        
        mean    0.640
        writing_score  0.640
    
    Returns:
        {"score": ì£¼ìš”ì ìˆ˜, "details": {ë©”íŠ¸ë¦­ëª…: ê°’, ...}}
    """
    all_metrics = {}
    
    # ëª¨ë“  "ì´ë¦„  ìˆ«ì" íŒ¨í„´ íŒŒì‹± (ì¤„ ì‹œì‘, ë°‘ì¤„/ì˜ë¬¸/ìˆ«ì ì´ë¦„)
    # stderrëŠ” ì œì™¸
    pattern = r"^([a-zA-Z][a-zA-Z0-9_]*)\s+([\d.-]+)\s*$"
    for match in re.finditer(pattern, output, re.MULTILINE):
        metric_name = match.group(1)
        # stderr, samples, tokens ë“± ë©”íƒ€ ì •ë³´ ì œì™¸
        if metric_name.lower() in ["stderr", "samples", "tokens", "total"]:
            continue
        try:
            all_metrics[metric_name] = float(match.group(2))
        except ValueError:
            pass
    
    if not all_metrics:
        return None
    
    # ì£¼ìš” ì ìˆ˜ ì„ íƒ
    main_score = None
    
    # IFEval: final_acc ë˜ëŠ” prompt_strict_acc ì‚¬ìš©
    if benchmark == "ifeval_ko":
        main_score = all_metrics.get("final_acc") or all_metrics.get("prompt_strict_acc")
    
    # KoBBQ: kobbq_avg ì‚¬ìš©
    elif benchmark == "kobbq":
        main_score = all_metrics.get("kobbq_avg")
    
    # HLE: hle_accuracy ì‚¬ìš©
    elif benchmark == "ko_hle":
        main_score = all_metrics.get("hle_accuracy") or all_metrics.get("accuracy")
    
    # HalluLens: correct_rate ë˜ëŠ” refusal_rate ì‚¬ìš©
    elif "hallulens" in benchmark:
        main_score = all_metrics.get("correct_rate") or all_metrics.get("refusal_rate")
    
    # MT-Bench: mean ì‚¬ìš© (10ì  ë§Œì  â†’ 0-1 ìŠ¤ì¼€ì¼)
    elif benchmark == "mtbench_ko":
        if "mean" in all_metrics:
            main_score = all_metrics["mean"] / 10.0
    
    # BFCL: accuracy ì‚¬ìš©
    elif benchmark == "bfcl":
        main_score = all_metrics.get("accuracy")
    
    # SQuAD: f1 > exact ìš°ì„ ìˆœìœ„
    elif benchmark == "squad_kor_v1":
        main_score = all_metrics.get("mean")  # f1.mean
    
    # ì¼ë°˜ì ì¸ ë©”íŠ¸ë¦­ ìš°ì„ ìˆœìœ„
    if main_score is None:
        for metric in ["accuracy", "mean", "macro_f1", "f1", "resolved"]:
            if metric in all_metrics:
                main_score = all_metrics[metric]
                if metric == "mean" and benchmark == "mtbench_ko":
                    main_score = main_score / 10.0  # mtbench ìŠ¤ì¼€ì¼
                break
    
    return {
        "score": main_score,
        "details": all_metrics,
    }


def main():
    parser = argparse.ArgumentParser(
        description="Run benchmarks and create leaderboard",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
    # ê¸°ë³¸ ì‹¤í–‰ (entity/projectëŠ” configs/base_config.yamlì—ì„œ ë¡œë“œ)
    uv run python run_eval.py --config gpt-4o

    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê°€ë²¼ìš´ ë²¤ì¹˜ë§ˆí¬ë§Œ)
    uv run python run_eval.py --config gpt-4o --quick
    
    # íŠ¹ì • ë²¤ì¹˜ë§ˆí¬ë§Œ ì‹¤í–‰
    uv run python run_eval.py --config gpt-4o --only ko_hellaswag,kmmlu
"""
    )
    
    # ê¸°ë³¸ ì˜µì…˜
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Model config name (configs/models/<name>.yaml, e.g., gpt-4o, solar_pro2)",
    )
    parser.add_argument("--limit", type=int,
                        help="Number of samples per benchmark")
    parser.add_argument("--quick", action="store_true",
                        help="Run only quick/light benchmarks")
    parser.add_argument("--only", type=str, default="",
                        help="Comma-separated list of benchmarks to run (exclusive)")
    
    args = parser.parse_args()
    
    # base_config.yamlì—ì„œ W&B ì„¤ì • ë¡œë“œ
    config = get_config()
    wandb_config = config.wandb
    
    entity = wandb_config.get("entity", "")
    project = wandb_config.get("project", "")
    
    if not entity or not project:
        print("âŒ W&B ë¡œê¹…ì„ ìœ„í•´ entityì™€ projectê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   configs/base_config.yamlì˜ wandb ì„¹ì…˜ì— ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # ë²¤ì¹˜ë§ˆí¬ í•„í„°ë§
    if args.quick:
        benchmarks = QUICK_BENCHMARKS
    elif args.only:
        benchmarks = [b.strip() for b in args.only.split(",") if b.strip()]
        # ìœ íš¨ì„± ê²€ì‚¬
        invalid = [b for b in benchmarks if b not in ALL_BENCHMARKS]
        if invalid:
            print(f"âŒ Unknown benchmarks: {invalid}")
            print(f"   Available: {ALL_BENCHMARKS}")
            sys.exit(1)
    else:
        benchmarks = ALL_BENCHMARKS
    
    # ëª¨ë¸ ì„¤ì • ë¡œë“œ (configs/models/<name>.yaml)
    model_cfg = config.get_model(args.config)
    if not model_cfg:
        print(f"âŒ ëª¨ë¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        print("   configs/models/ ë””ë ‰í† ë¦¬ì— YAML íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    model_id = model_cfg.get("model_id") or args.config

    # í‘œì‹œìš© ëª¨ë¸ ì´ë¦„ (openai/solar-pro2 â†’ solar-pro2)
    model_name = model_id.split("/")[-1] if "/" in model_id else model_id
    
    wandb_run = wandb.init(
        entity=entity,
        project=project,
        name=f"eval-{model_name}-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        job_type="evaluation",
        config={
            "config": args.config,
            "model": model_id,
            "model_name": model_name,
            "limit": args.limit,
            "benchmarks": benchmarks,
        },
    )
    print(f"âœ… W&B run ì‹œì‘: {wandb_run.url}")
    
    
    print(f"\nğŸ¯ Horangi Benchmark Runner")
    print(f"{'='*60}")
    print(f"Config: {args.config}")
    print(f"Model: {model_id}")
    print(f"Limit: {args.limit} samples per benchmark")
    print(f"Benchmarks: {len(benchmarks)} / {len(ALL_BENCHMARKS)}")
    print(f"Leaderboard: {entity}/{project}")
    print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    # ì‹¤í–‰ ê²°ê³¼ ì¶”ì 
    results = []
    benchmark_scores = {}
    
    for i, benchmark in enumerate(benchmarks, 1):
        print(f"\n[{i}/{len(benchmarks)}] ", end="")
        name, success, error, scores = run_benchmark(
            benchmark, 
            args.config,
            args.limit,
            wandb_entity=entity,
            wandb_project=project,
        )
        results.append((name, success, error))
        
        if scores:
            benchmark_scores[name] = scores
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n\n{'='*60}")
    print(f"ğŸ“Š Results Summary")
    print(f"{'='*60}")
    
    successful = [r for r in results if r[1]]
    failed = [r for r in results if not r[1]]
    
    print(f"\nâœ… Successful: {len(successful)} / {len(results)}")
    for name, _, _ in successful:
        score_info = benchmark_scores.get(name, {})
        score_str = f" (score: {score_info.get('score', 'N/A')})" if score_info else ""
        print(f"   - {name}{score_str}")
    
    if failed:
        print(f"\nâŒ Failed: {len(failed)} / {len(results)}")
        for name, _, error in failed:
            print(f"   - {name}: {error}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ Detailed Results by Category")
    print(f"{'='*60}")
    
    for benchmark_name, score_info in benchmark_scores.items():
        details = score_info.get("details", {})
        if len(details) > 1:  # ìƒì„¸ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ë§Œ
            print(f"\nğŸ“Œ {benchmark_name}")
            print(f"   {'â”€'*40}")
            
            # ì£¼ìš” ë©”íŠ¸ë¦­ê³¼ ì¹´í…Œê³ ë¦¬ë³„ ë©”íŠ¸ë¦­ êµ¬ë¶„
            main_metrics = []
            category_metrics = []
            
            for metric, value in sorted(details.items()):
                if "_score" in metric or "_accuracy" in metric or "_rate" in metric or "_acc" in metric:
                    category_metrics.append((metric, value))
                else:
                    main_metrics.append((metric, value))
            
            # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶œë ¥
            for metric, value in main_metrics:
                print(f"   {metric:<30} {value:.4f}")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë©”íŠ¸ë¦­ ì¶œë ¥ (í…Œì´ë¸” í˜•ì‹)
            if category_metrics:
                print(f"   {'â”€'*40}")
                for metric, value in category_metrics:
                    print(f"   {metric:<30} {value:.4f}")
    
    # Weave Leaderboard ìƒì„± (ì„±ê³µí•œ ë²¤ì¹˜ë§ˆí¬ê°€ ìˆìœ¼ë©´)
    if benchmark_scores and entity and project:
        try:
            from core.weave_leaderboard import create_weave_leaderboard
            # ì„±ê³µí•œ ë²¤ì¹˜ë§ˆí¬ ëª©ë¡ë§Œ ì „ë‹¬
            successful_benchmarks = list(benchmark_scores.keys())
            leaderboard_url = create_weave_leaderboard(
                entity=entity,
                project=project,
                benchmarks=successful_benchmarks,
            )
            if leaderboard_url:
                print(f"\nğŸ† Leaderboard URL: {leaderboard_url}")
        except Exception as e:
            print(f"âš ï¸ Weave Leaderboard ìƒì„± ì‹¤íŒ¨: {e}")
    
    # W&B run ì¢…ë£Œ
    if wandb_run is not None:
        print(f"\nğŸ“Š W&B run ì¢…ë£Œ ì¤‘...")
        wandb_run.finish()
        print(f"âœ… W&B run ì™„ë£Œ!")
    
    print(f"\n{'='*60}")
    print(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    # ì‹¤íŒ¨ê°€ ìˆì–´ë„ ë¦¬ë”ë³´ë“œê°€ ìƒì„±ë˜ì—ˆìœ¼ë©´ exit code 0
    # (ì¼ë¶€ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨í•´ë„ ê²°ê³¼ëŠ” ì €ì¥ë¨)
    sys.exit(0)


if __name__ == "__main__":
    main()
